{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which weights to choose? 1:normal 2:lle 3:iso1\n",
      "61560\n",
      "[0 0 0 ... 1 1 1]\n",
      "data loaded\n",
      "25206\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"]='tensorflow'\n",
    "#import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "weight_choice = input('Which weights to choose? 1:normal 2:lle 3:iso')\n",
    "\n",
    "\n",
    "\n",
    "#from multiprocessing import Pool\n",
    "#import multiprocessing as multi\n",
    "#from data.func import load_npy, padding_mat\n",
    "#sys.path.append('C:\\\\ProgramData\\\\Anaconda3\\\\pkgs\\\\pydot-1.2.3-py36hd4f83f9_0\\\\Lib\\\\site-packages')\n",
    "#sys.path\n",
    "x_train = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/train_data_features_sorted.npy')#np.load('/home/owner/デスクトップ/PythonFile/imdb/x_train_sort.npy')\n",
    "y_train = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/train_data_scores_binary.npy')#np.load('/home/owner/デスクトップ/PythonFile/imdb/x_test_sort.npy')\n",
    "#print(x_test)\n",
    "\n",
    "x_test = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/test_data_features_sorted.npy')#np.load('/home/owner/デスクトップ/PythonFile/imdb/t_train.npy')\n",
    "y_test = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/test_data_scores_binary.npy')#np.load('/home/owner/デスクトップ/PythonFile/imdb/t_test.npy')\n",
    "\n",
    "x_valid = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/valid_data_features_sorted.npy')\n",
    "y_valid = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/validation_data_scores_binary.npy')\n",
    "\n",
    "train_idx = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/train_data_idx_binary_only.npy')\n",
    "test_idx = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/test_data_idx_binary_only.npy')\n",
    "valid_idx = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/validation_data_idx_binary_only.npy')\n",
    "\n",
    "#print(len(train_idx))\n",
    "#x_train = x_train[train_idx]\n",
    "#y_train = y_train[train_idx]\n",
    "#print(len(x_train))\n",
    "#print(len(y_train))\n",
    "\n",
    "#x_test = x_test[test_idx]\n",
    "#y_test = y_test[test_idx]\n",
    "\n",
    "#x_valid = x_valid[valid_idx]\n",
    "#y_valid = y_valid[valid_idx]\n",
    "\n",
    "#print(x_valid)\n",
    "\n",
    "word_idx=np.load( '/home/owner/デスクトップ/milnet+edu/data/Preprocessed/' + 'vocab_idx.npy')\n",
    "#print(idx)\n",
    "#mani_choice = input('Which weights to choose? 1:normal 2:lle 3:iso')\n",
    "if (int(weight_choice)==1):\n",
    "    embWeights=np.load( '/home/owner/デスクトップ/milnet+edu/data/Preprocessed/' + 'weights.npy')#np.load('/home/owner/デスクトップ/PythonFile/imdb/weights.npy')\n",
    "elif (int(weight_choice)==2):\n",
    "    embWeights=np.load( '/home/owner/デスクトップ/milnet+edu/data/manifold_weights/' + 'weights_LLE1_1000_20_300.npy')#np.load('/home/owner/デスクトップ/PythonFile/imdb/weights.npy')\n",
    "elif (int(weight_choice)==3):\n",
    "     embWeights=np.load( '/home/owner/デスクトップ/milnet+edu/data/manifold_weights/' + 'weights_Isomap_1_1000_20_300.npy')\n",
    "\n",
    "print(len(word_idx))\n",
    "print(y_test)\n",
    "\n",
    "print('data loaded')\n",
    "\n",
    "features = x_valid\n",
    "print(len(features))\n",
    "for i in range(len(features)):\n",
    "    #print(len(features[i]))\n",
    "    if len(features[i])!=40:\n",
    "            print('i:'+str(i)+'   ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "            print(len(features[i]))\n",
    "            x_valid[i].append(x_valid[i][-1])\n",
    "    for j in range(len(features[i])):\n",
    "        \n",
    "        #print(len(features[i][j]))\n",
    "        if len(features[i][j])!= 15:\n",
    "            print('i:'+str(i)+'j:'+str(j)+'   ************************************************************')\n",
    "\n",
    "x_valid=np.asarray(x_valid.tolist())\n",
    "##\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, merge\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda, regularizers, Average\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Conv1D, MaxPooling2D, GlobalMaxPooling2D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers.core import Dropout, Dense, Lambda, Masking\n",
    "from keras.layers import merge, Layer, Activation, Dot, Concatenate, Flatten, Lambda\n",
    "\n",
    "from keras.initializers import Identity,glorot_normal\n",
    "from keras import regularizers\n",
    "\n",
    "from keras import metrics\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210642, 40, 15) (25510, 40, 15) (25206, 40, 15)\n",
      "(210642,) (25510,) (25206,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape, x_valid.shape)\n",
    "print(y_train.shape, y_test.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 15\n",
      "61560 300\n",
      "Attention Model Build Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:115: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n##\\nprint('Train...')\\nhistory = model.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=epochs\\n                    ,validation_split=0.2, shuffle=True)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numSentencesPerDoc, numWordsPerSentence = x_train[0].shape[0], x_train[0].shape[1]\n",
    "print(numSentencesPerDoc, numWordsPerSentence)\n",
    "#print(x_train[0])\n",
    "\n",
    "vocabSize, embeddingSize = embWeights.shape[0], embWeights.shape[1]\n",
    "print(vocabSize, embeddingSize)\n",
    "\n",
    "dropWordEmb = 0.25\n",
    "recursiveClass = GRU\n",
    "\n",
    "filters = 100 #embeddingSize*2\n",
    "windowMin = 3\n",
    "windowMax = 6# dimOfSentimentMetrics = 5\n",
    "batch_size = 200\n",
    "#epochs = 25\n",
    "dimGRU = 50\n",
    "numDensePool=10\n",
    "eta = 1e-4\n",
    "dr = 0.5\n",
    "\n",
    "##\n",
    "\n",
    "#wordsInputs = Input(shape=(numWordsPerSentence,1), batch_shape=(numSentencesPerDoc,numWordsPerSentence,), dtype='int32', name='words_input')\n",
    "\n",
    "x_in = Input( shape = ( numSentencesPerDoc, numWordsPerSentence ) , name='Input' )\n",
    "#x_pop = Lambda( lambda x: x, output_shape=(numWordsPerSentence, ) , name='convert_shape' )( x_in )\n",
    "    \n",
    "#Layer functionの定義\n",
    "embLayer = Embedding( input_dim=embWeights.shape[0], output_dim=embWeights.shape[1], weights=[embWeights]\n",
    "                      ,mask_zero=False , trainable=False, embeddings_regularizer=regularizers.l2(0.0000001)\n",
    "                      , input_length=numWordsPerSentence, name='Embedding' )\n",
    "\n",
    "\n",
    "maxPooledPerDoc = []\n",
    "convNets = []\n",
    "maxPools = []\n",
    "\n",
    "extraDimLayer = Lambda(lambda x: K.expand_dims(x), name='extraDimForConvo')\n",
    "squeezeThirdLayer = Lambda(lambda x: K.squeeze(x, 3), name='squeezeThirdLayer')\n",
    "\n",
    "for windowSize in range(windowMin,windowMax):\n",
    "    name='word_mat_convo_win_size_'+str(windowSize)\n",
    "    #convNet = Conv2D(filters, kernel_size=(windowSize,embeddingSize), padding='valid', activation='relu'\n",
    "    #                 ,strides=1, use_bias=True, input_shape=(numWordsPerSentence, embeddingSize, 1), data_format=\"channels_last\",kernel_initializer=glorot_normal()\n",
    "    #                 ,bias_regularizer=regularizers.l2(eta), kernel_regularizer=regularizers.l2(eta),name=name)\n",
    "    convNet = Conv1D(filters=filters, kernel_size=windowSize, padding='valid', activation='relu', strides=1)\n",
    "    convNets.append(convNet)\n",
    "    name='word_mat_max_pool_win_size_'+str(windowSize)\n",
    "    maxPool = MaxPooling1D(pool_size = int(numWordsPerSentence-windowSize-1), padding='valid')\n",
    "    maxPools.append(maxPool)\n",
    "    \n",
    "    \n",
    "for i in range(numSentencesPerDoc):\n",
    "    maxPooledPerSentence = []\n",
    "    x_pop = Lambda(lambda x: x[:,i], output_shape=(numWordsPerSentence, ) , name='convert_shape_'+'sentence'+str(i+1))( x_in )\n",
    "\n",
    "    for j in range(windowMax-windowMin):   \n",
    "        emb = embLayer(x_pop)\n",
    "        #emb = Dropout(dr,name='DropEmb'+str(i)+str(j))(emb)\n",
    "        #reshaped = extraDimLayer(emb)#Lambda(lambda x: K.expand_dims(x), name='extraDimForConvo_'+str(j)+'_sentence_'+str(i))(emb)\n",
    "        #name='word_mat_convo_win_size_'+str(j)+'_sentence_'+str(i)\n",
    "        #wordsCNN = Conv2D(filters, kernel_size=(windowSize,embeddingSize), padding='valid', \n",
    "        #                    activation='relu', strides=1, use_bias=True, input_shape=(numWordsPerSentence, embeddingSize, 1), data_format=\"channels_last\",\n",
    "        #                    kernel_initializer=glorot_normal(),kernel_regularizer=regularizers.l2(),name=name)(reshaped)\n",
    "        wordsCNN  = convNets[j](emb)\n",
    "        #wordsCNN = Dropout(dr,name='DropCNN'+str(i)+str(j))(wordsCNN)\n",
    "        #squeezed = squeezeThirdLayer(wordsCNN)#Lambda(lambda x: K.squeeze(x, 3), name='squeezeThirdLayer_'+str(j)+'_sentence_'+str(i))(wordsCNN)\n",
    "        # newShape = (-1, int(squeezed.shape[1])*int(squeezed.shape[2]))\n",
    "        # squeezed = Lambda(lambda x: K.reshape(x,shape=newShape), name ='squeezeDimForMaxPool'+str(i)+str(j))(squeezed)\n",
    "        #wordsCNNPooled=GlobalMaxPooling1D()(squeezed)\n",
    "        #wordsCNNPooled= MaxPooling1D(pool_size = int(squeezed.shape[1]), padding='valid')(squeezed)\n",
    "        wordsCNNPooled = MaxPooling1D(pool_size=(numWordsPerSentence-(j+windowMin)+1))(wordsCNN)\n",
    "        flattened = Lambda(lambda x: K.squeeze(x, 1))(wordsCNNPooled)\n",
    "        maxPooledPerSentence.append(flattened)\n",
    "        \n",
    "    mergedPoolForSentence = Concatenate(axis = 1)(maxPooledPerSentence)\n",
    "    newShape=(-1,1,int(mergedPoolForSentence.shape[1]))\n",
    "    reshapedPoolForSentence = Lambda(lambda x: K.reshape(x,shape=newShape), name ='switch_axis_'+'sentence'+str(i+1)+'winSize'+str(j+windowMin))(mergedPoolForSentence)\n",
    "    densePoolForSentence = Dense(numDensePool, bias_regularizer=regularizers.l2(eta),\n",
    "                                 kernel_regularizer=regularizers.l2(eta), activation='softmax', use_bias=True)(reshapedPoolForSentence)\n",
    "\n",
    "    densePoolForSentence = Dropout(dr,name='DropDense'+str(i))(densePoolForSentence)\n",
    "    maxPooledPerDoc.append(densePoolForSentence)\n",
    "    \n",
    "#Naive Approach\n",
    "averaged = Average()(maxPooledPerDoc) \n",
    "averaged = Lambda(lambda x:K.reshape(x,shape=(-1,int(averaged.shape[1])*int(averaged.shape[2]))), name ='attend_output')(averaged)\n",
    "out_avg = Dense(1, activation='sigmoid', use_bias=True)(averaged) \n",
    "    \n",
    "#Apply Attention \n",
    "mergedPoolPerDoc = Concatenate(axis = 1)(maxPooledPerDoc)\n",
    "biRnn_ = Bidirectional(GRU(dimGRU,  return_sequences=True), merge_mode='concat')(mergedPoolPerDoc)\n",
    "newShape = (-1, int(mergedPoolPerDoc.shape[1]), int(biRnn_.shape[2]))\n",
    "biRnn = Lambda(lambda x: K.reshape(x,shape=newShape), name ='biRnn_TF_Reminder1')(biRnn_)\n",
    "#biRnn2 = Lambda(lambda x: K.reshape(x,shape=newShape), name ='biRnn_TF_Reminder2')(biRnn_[1])\n",
    "\n",
    "#biRnn_cat = Concatenate(axis = 2)([biRnn1, biRnn2])\n",
    "\n",
    "CONTEXT_DIM = 100\n",
    "\n",
    "eij = Dense(CONTEXT_DIM, use_bias=True, activation='tanh')(biRnn)\n",
    "eij = Dense(1, use_bias=False, activation='softmax')(eij)\n",
    "\n",
    "weighted_input = Dot(axes = 1)([eij, mergedPoolPerDoc])\n",
    "#weighted_input = Lambda(lambda x: K.reshape(x,shape=(-1,int(weighted_input_.shape[1])*int(weighted_input_.shape[2]))), name ='attend_output')(weighted_input_)\n",
    "weighted_input = Lambda(lambda x: K.squeeze(x, 1), name='squeezeOutput')(weighted_input)\n",
    "\n",
    "out = Dense(1, activation='sigmoid', use_bias=True)(weighted_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "model = Model(input=[x_in], output=[out])\n",
    "#adadelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "#model.compile(loss='binary_crossentropy',\n",
    "#              optimizer=adadelta,\n",
    "#              metrics=['accuracy'])\n",
    "         \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Attention Model Build Complete\")\n",
    "##\n",
    "#model_avg = Model(inputs=[x_in], outputs=[out_avg])\n",
    "#model_avg.compile(loss='binary_crossentropy',\n",
    "#              optimizer=keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "#print(\"Average Model Build Complete\")\n",
    "##\n",
    "#save model to png file\n",
    "#from keras.utils import plot_model\n",
    "#plot_model( model, to_file='model.png' )\n",
    "\n",
    "#モデルを保存せず直接可視化\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "#SVG( model_to_dot( model ).create( prog='dot', format='svg' ) )\n",
    "'''\n",
    "##\n",
    "print('Train...')\n",
    "history = model.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=epochs\n",
    "                    ,validation_split=0.2, shuffle=True)\n",
    "'''\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = './params_milnet_adam_0720_2_adjCNN_fixed_weights.hdf5'\n",
    "save_model = keras.callbacks.ModelCheckpoint(name, monitor='val_loss', verbose=1\n",
    "                                             , save_best_only=True, save_weights_only=True, mode='min', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 210642 samples, validate on 25206 samples\n",
      "Epoch 1/40\n",
      "210642/210642 [==============================] - 286s 1ms/step - loss: 0.6410 - acc: 0.7789 - val_loss: 0.5129 - val_acc: 0.8406\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51290, saving model to ./params_milnet_adam_0720_2_adjCNN_fixed_weights.hdf5\n",
      "Epoch 2/40\n",
      "210642/210642 [==============================] - 283s 1ms/step - loss: 0.5230 - acc: 0.8336 - val_loss: 0.5001 - val_acc: 0.8293\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51290 to 0.50006, saving model to ./params_milnet_adam_0720_2_adjCNN_fixed_weights.hdf5\n",
      "Epoch 3/40\n",
      "210642/210642 [==============================] - 281s 1ms/step - loss: 0.4872 - acc: 0.8468 - val_loss: 0.4644 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.50006 to 0.46436, saving model to ./params_milnet_adam_0720_2_adjCNN_fixed_weights.hdf5\n",
      "Epoch 4/40\n",
      "210642/210642 [==============================] - 281s 1ms/step - loss: 0.4650 - acc: 0.8546 - val_loss: 0.4480 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46436 to 0.44805, saving model to ./params_milnet_adam_0720_2_adjCNN_fixed_weights.hdf5\n",
      "Epoch 5/40\n",
      "210642/210642 [==============================] - 281s 1ms/step - loss: 0.4484 - acc: 0.8605 - val_loss: 0.4270 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.44805 to 0.42696, saving model to ./params_milnet_adam_0720_2_adjCNN_fixed_weights.hdf5\n",
      "Epoch 6/40\n",
      "210642/210642 [==============================] - 280s 1ms/step - loss: 0.4358 - acc: 0.8662 - val_loss: 0.4285 - val_acc: 0.8726\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/40\n",
      "210642/210642 [==============================] - 281s 1ms/step - loss: 0.4238 - acc: 0.8709 - val_loss: 0.4304 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/40\n",
      "210642/210642 [==============================] - 280s 1ms/step - loss: 0.4155 - acc: 0.8742 - val_loss: 0.4265 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.42696 to 0.42652, saving model to ./params_milnet_adam_0720_2_adjCNN_fixed_weights.hdf5\n",
      "Epoch 9/40\n",
      "210642/210642 [==============================] - 280s 1ms/step - loss: 0.4058 - acc: 0.8795 - val_loss: 0.4238 - val_acc: 0.8749\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.42652 to 0.42377, saving model to ./params_milnet_adam_0720_2_adjCNN_fixed_weights.hdf5\n",
      "Epoch 10/40\n",
      "210642/210642 [==============================] - 281s 1ms/step - loss: 0.3969 - acc: 0.8837 - val_loss: 0.5015 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/40\n",
      "210642/210642 [==============================] - 281s 1ms/step - loss: 0.3881 - acc: 0.8890 - val_loss: 0.4183 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.42377 to 0.41835, saving model to ./params_milnet_adam_0720_2_adjCNN_fixed_weights.hdf5\n",
      "Epoch 12/40\n",
      "210642/210642 [==============================] - 281s 1ms/step - loss: 0.3782 - acc: 0.8945 - val_loss: 0.4553 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/40\n",
      "  6400/210642 [..............................] - ETA: 4:27 - loss: 0.3576 - acc: 0.9045"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f503c4727a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=40 #epochs\n\u001b[0;32m----> 2\u001b[0;31m                         ,validation_data=(x_valid,y_valid), shuffle=True, callbacks=[save_model])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=40 #epochs\n",
    "                        ,validation_data=(x_valid,y_valid), shuffle=True, callbacks=[save_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(name)#('./params_milnet_adam_0718_2_fixed_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 210642 samples, validate on 25206 samples\n",
      "Epoch 51/200\n",
      "210642/210642 [==============================] - 235s 1ms/step - loss: 0.5979 - acc: 0.7541 - val_loss: 0.5779 - val_acc: 0.7929\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/200\n",
      "210642/210642 [==============================] - 236s 1ms/step - loss: 0.5970 - acc: 0.7544 - val_loss: 0.5791 - val_acc: 0.7942\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/200\n",
      "210642/210642 [==============================] - 235s 1ms/step - loss: 0.5976 - acc: 0.7535 - val_loss: 0.5818 - val_acc: 0.7967\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/200\n",
      "210432/210642 [============================>.] - ETA: 0s - loss: 0.5973 - acc: 0.7540"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1464adf59c34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=200 #epochs\n\u001b[0;32m----> 2\u001b[0;31m                         ,initial_epoch=50, validation_data=(x_valid,y_valid), shuffle=True, callbacks=[save_model])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1247\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1248\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1250\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1431\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=200 #epochs\n",
    "                        ,initial_epoch=50, validation_data=(x_valid,y_valid), shuffle=True, callbacks=[save_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model_avg.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=300 #epochs\n",
    "                        , validation_data=(x_valid,y_valid), shuffle=True, callbacks=[save_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot history\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    # 精度の履歴をプロット\n",
    "    plt.plot(history.history['acc'],label=\"accuracy\")\n",
    "    plt.plot(history.history['val_acc'],\"o-\",label=\"val_acc\")\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # 損失の履歴をプロット\n",
    "    plt.plot(history.history['loss'],label=\"loss\",)\n",
    "    plt.plot(history.history['val_loss'],\"o-\",label=\"val_loss\")\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b61019f67e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_history' is not defined"
     ]
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('./params_milnet_adam_0717_1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25510/25510 [==============================] - 40s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "score=model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3631319922776561 0.8815758526068208\n"
     ]
    }
   ],
   "source": [
    "print(score[0],score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/train_data_features_sorted.npy', x_train)\n",
    "np.save('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/test_data_features_sorted.npy', x_test)\n",
    "np.save('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/valid_data_features_sorted.npy', x_valid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
