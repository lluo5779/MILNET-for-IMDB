{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/owner/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 50\n",
      "52306 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:166: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Model Build Complete\n",
      "Average Model Build Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n##\\nprint('Train...')\\nhistory = model.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=epochs\\n                    ,validation_split=0.2, shuffle=True)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"]='tensorflow'\n",
    "#import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#from multiprocessing import Pool\n",
    "#import multiprocessing as multi\n",
    "#from data.func import load_npy, padding_mat\n",
    "#sys.path.append('C:\\\\ProgramData\\\\Anaconda3\\\\pkgs\\\\pydot-1.2.3-py36hd4f83f9_0\\\\Lib\\\\site-packages')\n",
    "#sys.path\n",
    "x_train = np.load('/home/owner/デスクトップ/PythonFile/imdb/x_train_sort.npy')\n",
    "x_test = np.load('/home/owner/デスクトップ/PythonFile/imdb/x_test_sort.npy')\n",
    "y_train = np.load('/home/owner/デスクトップ/PythonFile/imdb/t_train.npy')\n",
    "y_test = np.load('/home/owner/デスクトップ/PythonFile/imdb/t_test.npy')\n",
    "\n",
    "embWeights=np.load('/home/owner/デスクトップ/PythonFile/imdb/weights.npy')\n",
    "idx=np.load('/home/owner/デスクトップ/PythonFile/imdb/index.npy')\n",
    "embWeights = embWeights[idx]\n",
    "\n",
    "print('data loaded')\n",
    "\n",
    "##\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, merge\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda, regularizers, Average\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Conv1D, MaxPooling2D, GlobalMaxPooling2D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers.core import Dropout, Dense, Lambda, Masking\n",
    "from keras.layers import merge, Layer, Activation, Dot, Concatenate, Flatten, Lambda\n",
    "\n",
    "from keras.initializers import Identity,glorot_normal\n",
    "from keras import regularizers\n",
    "\n",
    "from keras import metrics\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "\n",
    "numSentencesPerDoc, numWordsPerSentence = x_train[0].shape[0], x_train[0].shape[1]\n",
    "print(numSentencesPerDoc, numWordsPerSentence)\n",
    "#print(x_train[0])\n",
    "\n",
    "vocabSize, embeddingSize = embWeights.shape[0], embWeights.shape[1]\n",
    "print(vocabSize, embeddingSize)\n",
    "\n",
    "dropWordEmb = 0.25\n",
    "recursiveClass = GRU\n",
    "\n",
    "filters = 1 #embeddingSize*2\n",
    "windowMin = 3\n",
    "windowMax = 7\n",
    "# dimOfSentimentMetrics = 5\n",
    "batch_size = 256\n",
    "epochs = 25\n",
    "numGRU = 100\n",
    "numDensePool=50\n",
    "eta = 1e-5\n",
    "dr = 0.5\n",
    "\n",
    "##\n",
    "\n",
    "#wordsInputs = Input(shape=(numWordsPerSentence,1), batch_shape=(numSentencesPerDoc,numWordsPerSentence,), dtype='int32', name='words_input')\n",
    "\n",
    "x_in = Input( shape = ( numSentencesPerDoc, numWordsPerSentence ) , name='Input' )\n",
    "#x_pop = Lambda( lambda x: x, output_shape=(numWordsPerSentence, ) , name='convert_shape' )( x_in )\n",
    "    \n",
    "#Layer functionの定義\n",
    "embLayer = Embedding( input_dim=embWeights.shape[0], output_dim=embWeights.shape[1], weights=[embWeights]\n",
    "                      ,mask_zero=True , trainable=True, embeddings_regularizer=regularizers.l2(0.0000001)\n",
    "                      , input_length=numWordsPerSentence, name='Embedding' )\n",
    "\n",
    "\n",
    "maxPooledPerDoc = []\n",
    "convNets = []\n",
    "maxPools = []\n",
    "\n",
    "extraDimLayer = Lambda(lambda x: K.expand_dims(x), name='extraDimForConvo')\n",
    "squeezeThirdLayer = Lambda(lambda x: K.squeeze(x, 3), name='squeezeThirdLayer')\n",
    "\n",
    "for windowSize in range(windowMin,windowMax):\n",
    "    name='word_mat_convo_win_size_'+str(windowSize)\n",
    "    convNet = Conv2D(filters, kernel_size=(windowSize,embeddingSize), padding='valid', activation='relu'\n",
    "                     ,strides=1, use_bias=True, input_shape=(numWordsPerSentence, embeddingSize, 1), data_format=\"channels_last\",kernel_initializer=glorot_normal()\n",
    "                     ,bias_regularizer=regularizers.l2(eta), kernel_regularizer=regularizers.l2(eta),name=name)\n",
    "    convNets.append(convNet)\n",
    "    name='word_mat_max_pool_win_size_'+str(windowSize)\n",
    "    maxPool = MaxPooling1D(pool_size = int(numWordsPerSentence-windowSize-1), padding='valid')\n",
    "    maxPools.append(maxPool)\n",
    "    \n",
    "    \n",
    "for i in range(numSentencesPerDoc):\n",
    "    maxPooledPerSentence = []\n",
    "    x_pop = Lambda(lambda x: x[:,i], output_shape=(numWordsPerSentence, ) , name='convert_shape_'+'sentence'+str(i+1))( x_in )\n",
    "\n",
    "    for j in range(windowMax-windowMin):   \n",
    "        emb = embLayer(x_pop)\n",
    "        emb = Dropout(dr,name='DropEmb'+str(i)+str(j))(emb)\n",
    "        reshaped = extraDimLayer(emb)#Lambda(lambda x: K.expand_dims(x), name='extraDimForConvo_'+str(j)+'_sentence_'+str(i))(emb)\n",
    "        name='word_mat_convo_win_size_'+str(j)+'_sentence_'+str(i)\n",
    "        # wordsCNN = Conv2D(filters, kernel_size=(windowSize,embeddingSize), padding='valid', \n",
    "        #                    activation='relu', strides=1, use_bias=True, input_shape=(numWordsPerSentence, embeddingSize, 1), data_format=\"channels_last\",\n",
    "        #                    kernel_initializer=glorot_normal(),kernel_regularizer=regularizers.l2(),name=name)(reshaped)\n",
    "        wordsCNN  = convNets[j](reshaped)\n",
    "        wordsCNN = Dropout(dr,name='DropCNN'+str(i)+str(j))(wordsCNN)\n",
    "        squeezed = squeezeThirdLayer(wordsCNN)#Lambda(lambda x: K.squeeze(x, 3), name='squeezeThirdLayer_'+str(j)+'_sentence_'+str(i))(wordsCNN)\n",
    "        # newShape = (-1, int(squeezed.shape[1])*int(squeezed.shape[2]))\n",
    "        # squeezed = Lambda(lambda x: K.reshape(x,shape=newShape), name ='squeezeDimForMaxPool'+str(i)+str(j))(squeezed)\n",
    "        wordsCNNPooled=GlobalMaxPooling1D()(squeezed)\n",
    "        #wordsCNNPooled= MaxPooling1D(pool_size = int(squeezed.shape[1]), padding='valid')(squeezed)\n",
    "        maxPooledPerSentence.append(wordsCNNPooled)\n",
    "        \n",
    "    mergedPoolForSentence = Concatenate(axis = 1)(maxPooledPerSentence)\n",
    "    newShape=(-1,1,int(mergedPoolForSentence.shape[1]))\n",
    "    reshapedPoolForSentence = Lambda(lambda x: K.reshape(x,shape=newShape), name ='switch_axis_'+'sentence'+str(i+1)+'winSize'+str(j+windowMin))(mergedPoolForSentence)\n",
    "    densePoolForSentence = Dense(numDensePool, bias_regularizer=regularizers.l2(eta),\n",
    "                                 kernel_regularizer=regularizers.l2(eta), activation='softmax', use_bias=True)(reshapedPoolForSentence)\n",
    "\n",
    "    densePoolForSentence = Dropout(dr,name='DropDense'+str(i))(densePoolForSentence)\n",
    "    maxPooledPerDoc.append(densePoolForSentence)\n",
    "    \n",
    "#Naive Approach\n",
    "averaged = Average()(maxPooledPerDoc) \n",
    "averaged = Lambda(lambda x:K.reshape(x,shape=(-1,int(averaged.shape[1])*int(averaged.shape[2]))), name ='attend_output')(averaged)\n",
    "out_avg = Dense(1, activation='sigmoid', use_bias=True)(averaged) \n",
    "    \n",
    "#Apply Attention \n",
    "mergedPoolPerDoc = Concatenate(axis = 1)(maxPooledPerDoc)\n",
    "biRnn_ = Bidirectional(GRU(int(mergedPoolPerDoc.shape[2]),  return_sequences=True, bias_regularizer=regularizers.l2(eta)\n",
    "                           ,kernel_regularizer=regularizers.l2(eta),recurrent_regularizer=regularizers.l2(eta)\n",
    "                           ,dropout=dr, recurrent_dropout=dr, unroll=True), merge_mode='concat')(mergedPoolPerDoc)\n",
    "newShape = (-1, int(mergedPoolPerDoc.shape[1]), 2*int(mergedPoolPerDoc.shape[2]))\n",
    "biRnn = Lambda(lambda x: K.reshape(x,shape=newShape), name ='biRnn_TF_Reminder')(biRnn_)\n",
    "\n",
    "CONTEXT_DIM = 100#int(int(biRnn.shape[1])*int(biRnn.shape[2])/2) \n",
    "\n",
    "eij = Dense(CONTEXT_DIM, kernel_regularizer=regularizers.l2(eta), bias_regularizer=regularizers.l2(eta)\n",
    "            , use_bias=True, activation='tanh')(biRnn)\n",
    "eij = Dense(CONTEXT_DIM, kernel_regularizer=regularizers.l2(eta), use_bias=False, activation='softmax')(eij)\n",
    "\n",
    "weighted_input_ = Dot(axes = 1)([eij, biRnn])\n",
    "weighted_input = Lambda(lambda x: K.reshape(x,shape=(-1,int(weighted_input_.shape[1])*int(weighted_input_.shape[2]))), name ='attend_output')(weighted_input_)\n",
    "\n",
    "out = Dense(1, kernel_regularizer=regularizers.l2(eta), bias_regularizer=regularizers.l2(eta)\n",
    "            , activation='sigmoid', use_bias=True)(weighted_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "model = Model(input=[x_in], output=[out])\n",
    "# adadelta = keras.optimizers.Adadelta(lr=0.5, rho=0.95, epsilon=None, decay=0.0)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer=adadelta,\n",
    "#               metrics=['accuracy'])\n",
    "         \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Attention Model Build Complete\")\n",
    "##\n",
    "model_avg = Model(inputs=[x_in], outputs=[out_avg])\n",
    "model_avg.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Average Model Build Complete\")\n",
    "##\n",
    "#save model to png file\n",
    "from keras.utils import plot_model\n",
    "plot_model( model, to_file='model.png' )\n",
    "\n",
    "#モデルを保存せず直接可視化\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG( model_to_dot( model ).create( prog='dot', format='svg' ) )\n",
    "'''\n",
    "##\n",
    "print('Train...')\n",
    "history = model.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=epochs\n",
    "                    ,validation_split=0.2, shuffle=True)\n",
    "'''\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model = keras.callbacks.ModelCheckpoint('./params.hdf5', monitor='val_loss', verbose=1\n",
    "                                             , save_best_only=True, save_weights_only=True, mode='min', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19852 samples, validate on 4964 samples\n",
      "Epoch 1/300\n",
      "19852/19852 [==============================] - 18s 911us/step - loss: 0.7452 - acc: 0.5008 - val_loss: 0.7446 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.74463, saving model to ./params.hdf5\n",
      "Epoch 2/300\n",
      "19852/19852 [==============================] - 13s 656us/step - loss: 0.7435 - acc: 0.5008 - val_loss: 0.7428 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.74463 to 0.74281, saving model to ./params.hdf5\n",
      "Epoch 3/300\n",
      "19852/19852 [==============================] - 13s 660us/step - loss: 0.7416 - acc: 0.5008 - val_loss: 0.7409 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.74281 to 0.74091, saving model to ./params.hdf5\n",
      "Epoch 4/300\n",
      "19852/19852 [==============================] - 13s 656us/step - loss: 0.7398 - acc: 0.5008 - val_loss: 0.7390 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.74091 to 0.73903, saving model to ./params.hdf5\n",
      "Epoch 5/300\n",
      "19852/19852 [==============================] - 13s 651us/step - loss: 0.7379 - acc: 0.5009 - val_loss: 0.7371 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.73903 to 0.73715, saving model to ./params.hdf5\n",
      "Epoch 6/300\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.7361 - acc: 0.5008 - val_loss: 0.7353 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.73715 to 0.73529, saving model to ./params.hdf5\n",
      "Epoch 7/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.7342 - acc: 0.5011 - val_loss: 0.7335 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.73529 to 0.73346, saving model to ./params.hdf5\n",
      "Epoch 8/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.7323 - acc: 0.5015 - val_loss: 0.7316 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.73346 to 0.73162, saving model to ./params.hdf5\n",
      "Epoch 9/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.7303 - acc: 0.5028 - val_loss: 0.7297 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.73162 to 0.72974, saving model to ./params.hdf5\n",
      "Epoch 10/300\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.7282 - acc: 0.5025 - val_loss: 0.7278 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.72974 to 0.72777, saving model to ./params.hdf5\n",
      "Epoch 11/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.7256 - acc: 0.5045 - val_loss: 0.7256 - val_acc: 0.4950\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.72777 to 0.72559, saving model to ./params.hdf5\n",
      "Epoch 12/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.7227 - acc: 0.5151 - val_loss: 0.7231 - val_acc: 0.4968\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.72559 to 0.72309, saving model to ./params.hdf5\n",
      "Epoch 13/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.7190 - acc: 0.5683 - val_loss: 0.7202 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.72309 to 0.72016, saving model to ./params.hdf5\n",
      "Epoch 14/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.7140 - acc: 0.6716 - val_loss: 0.7165 - val_acc: 0.7456\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.72016 to 0.71648, saving model to ./params.hdf5\n",
      "Epoch 15/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.7076 - acc: 0.7378 - val_loss: 0.7118 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.71648 to 0.71182, saving model to ./params.hdf5\n",
      "Epoch 16/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.7006 - acc: 0.7413 - val_loss: 0.7067 - val_acc: 0.7808\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.71182 to 0.70671, saving model to ./params.hdf5\n",
      "Epoch 17/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.6937 - acc: 0.7519 - val_loss: 0.7018 - val_acc: 0.7911\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.70671 to 0.70175, saving model to ./params.hdf5\n",
      "Epoch 18/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.6877 - acc: 0.7567 - val_loss: 0.6971 - val_acc: 0.7977\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.70175 to 0.69710, saving model to ./params.hdf5\n",
      "Epoch 19/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.6824 - acc: 0.7656 - val_loss: 0.6928 - val_acc: 0.8052\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.69710 to 0.69279, saving model to ./params.hdf5\n",
      "Epoch 20/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.6776 - acc: 0.7716 - val_loss: 0.6888 - val_acc: 0.8098\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.69279 to 0.68884, saving model to ./params.hdf5\n",
      "Epoch 21/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.6716 - acc: 0.7843 - val_loss: 0.6848 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.68884 to 0.68481, saving model to ./params.hdf5\n",
      "Epoch 22/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.6679 - acc: 0.7770 - val_loss: 0.6812 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.68481 to 0.68122, saving model to ./params.hdf5\n",
      "Epoch 23/300\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.6635 - acc: 0.7921 - val_loss: 0.6777 - val_acc: 0.8213\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.68122 to 0.67771, saving model to ./params.hdf5\n",
      "Epoch 24/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.6589 - acc: 0.7937 - val_loss: 0.6744 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.67771 to 0.67437, saving model to ./params.hdf5\n",
      "Epoch 25/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.6553 - acc: 0.7962 - val_loss: 0.6713 - val_acc: 0.8253\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.67437 to 0.67126, saving model to ./params.hdf5\n",
      "Epoch 26/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.6525 - acc: 0.7955 - val_loss: 0.6685 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.67126 to 0.66848, saving model to ./params.hdf5\n",
      "Epoch 27/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.6493 - acc: 0.7982 - val_loss: 0.6656 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.66848 to 0.66559, saving model to ./params.hdf5\n",
      "Epoch 28/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.6453 - acc: 0.8023 - val_loss: 0.6629 - val_acc: 0.8288\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.66559 to 0.66294, saving model to ./params.hdf5\n",
      "Epoch 29/300\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.6426 - acc: 0.8017 - val_loss: 0.6603 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.66294 to 0.66029, saving model to ./params.hdf5\n",
      "Epoch 30/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.6392 - acc: 0.8052 - val_loss: 0.6577 - val_acc: 0.8308\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.66029 to 0.65771, saving model to ./params.hdf5\n",
      "Epoch 31/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.6365 - acc: 0.8069 - val_loss: 0.6552 - val_acc: 0.8340\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.65771 to 0.65523, saving model to ./params.hdf5\n",
      "Epoch 32/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.6337 - acc: 0.8113 - val_loss: 0.6530 - val_acc: 0.8332\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.65523 to 0.65295, saving model to ./params.hdf5\n",
      "Epoch 33/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.6306 - acc: 0.8072 - val_loss: 0.6510 - val_acc: 0.8280\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.65295 to 0.65095, saving model to ./params.hdf5\n",
      "Epoch 34/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.6288 - acc: 0.8123 - val_loss: 0.6485 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.65095 to 0.64853, saving model to ./params.hdf5\n",
      "Epoch 35/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.6262 - acc: 0.8136 - val_loss: 0.6466 - val_acc: 0.8304\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.64853 to 0.64665, saving model to ./params.hdf5\n",
      "Epoch 36/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.6233 - acc: 0.8117 - val_loss: 0.6443 - val_acc: 0.8362\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.64665 to 0.64434, saving model to ./params.hdf5\n",
      "Epoch 37/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.6207 - acc: 0.8145 - val_loss: 0.6424 - val_acc: 0.8358\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.64434 to 0.64240, saving model to ./params.hdf5\n",
      "Epoch 38/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.6188 - acc: 0.8192 - val_loss: 0.6406 - val_acc: 0.8360\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.64240 to 0.64062, saving model to ./params.hdf5\n",
      "Epoch 39/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.6161 - acc: 0.8145 - val_loss: 0.6388 - val_acc: 0.8362\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.64062 to 0.63882, saving model to ./params.hdf5\n",
      "Epoch 40/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.6140 - acc: 0.8201 - val_loss: 0.6371 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.63882 to 0.63711, saving model to ./params.hdf5\n",
      "Epoch 41/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.6124 - acc: 0.8181 - val_loss: 0.6354 - val_acc: 0.8394\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.63711 to 0.63535, saving model to ./params.hdf5\n",
      "Epoch 42/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.6104 - acc: 0.8224 - val_loss: 0.6338 - val_acc: 0.8382\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.63535 to 0.63380, saving model to ./params.hdf5\n",
      "Epoch 43/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.6089 - acc: 0.8190 - val_loss: 0.6321 - val_acc: 0.8398\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.63380 to 0.63210, saving model to ./params.hdf5\n",
      "Epoch 44/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.6069 - acc: 0.8218 - val_loss: 0.6305 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.63210 to 0.63051, saving model to ./params.hdf5\n",
      "Epoch 45/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.6046 - acc: 0.8232 - val_loss: 0.6292 - val_acc: 0.8394\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.63051 to 0.62920, saving model to ./params.hdf5\n",
      "Epoch 46/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.6025 - acc: 0.8244 - val_loss: 0.6274 - val_acc: 0.8433\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.62920 to 0.62740, saving model to ./params.hdf5\n",
      "Epoch 47/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.6007 - acc: 0.8247 - val_loss: 0.6260 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.62740 to 0.62604, saving model to ./params.hdf5\n",
      "Epoch 48/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.5991 - acc: 0.8247 - val_loss: 0.6247 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.62604 to 0.62471, saving model to ./params.hdf5\n",
      "Epoch 49/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.5969 - acc: 0.8276 - val_loss: 0.6232 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.62471 to 0.62320, saving model to ./params.hdf5\n",
      "Epoch 50/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5943 - acc: 0.8309 - val_loss: 0.6219 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.62320 to 0.62187, saving model to ./params.hdf5\n",
      "Epoch 51/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5926 - acc: 0.8339 - val_loss: 0.6205 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.62187 to 0.62048, saving model to ./params.hdf5\n",
      "Epoch 52/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5930 - acc: 0.8271 - val_loss: 0.6191 - val_acc: 0.8457\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.62048 to 0.61910, saving model to ./params.hdf5\n",
      "Epoch 53/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.5899 - acc: 0.8304 - val_loss: 0.6178 - val_acc: 0.8457\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.61910 to 0.61782, saving model to ./params.hdf5\n",
      "Epoch 54/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5883 - acc: 0.8349 - val_loss: 0.6166 - val_acc: 0.8435\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.61782 to 0.61661, saving model to ./params.hdf5\n",
      "Epoch 55/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.5852 - acc: 0.8381 - val_loss: 0.6152 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.61661 to 0.61523, saving model to ./params.hdf5\n",
      "Epoch 56/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.5855 - acc: 0.8318 - val_loss: 0.6140 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.61523 to 0.61403, saving model to ./params.hdf5\n",
      "Epoch 57/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5839 - acc: 0.8336 - val_loss: 0.6128 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.61403 to 0.61281, saving model to ./params.hdf5\n",
      "Epoch 58/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5823 - acc: 0.8341 - val_loss: 0.6116 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.61281 to 0.61161, saving model to ./params.hdf5\n",
      "Epoch 59/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5795 - acc: 0.8397 - val_loss: 0.6104 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.61161 to 0.61036, saving model to ./params.hdf5\n",
      "Epoch 60/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5776 - acc: 0.8430 - val_loss: 0.6094 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.61036 to 0.60937, saving model to ./params.hdf5\n",
      "Epoch 61/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5764 - acc: 0.8396 - val_loss: 0.6081 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.60937 to 0.60813, saving model to ./params.hdf5\n",
      "Epoch 62/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.5745 - acc: 0.8370 - val_loss: 0.6067 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.60813 to 0.60674, saving model to ./params.hdf5\n",
      "Epoch 63/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5732 - acc: 0.8456 - val_loss: 0.6058 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.60674 to 0.60578, saving model to ./params.hdf5\n",
      "Epoch 64/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5724 - acc: 0.8412 - val_loss: 0.6046 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.60578 to 0.60462, saving model to ./params.hdf5\n",
      "Epoch 65/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5712 - acc: 0.8398 - val_loss: 0.6035 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.60462 to 0.60346, saving model to ./params.hdf5\n",
      "Epoch 66/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5684 - acc: 0.8473 - val_loss: 0.6024 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.60346 to 0.60237, saving model to ./params.hdf5\n",
      "Epoch 67/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.5658 - acc: 0.8474 - val_loss: 0.6011 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.60237 to 0.60114, saving model to ./params.hdf5\n",
      "Epoch 68/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5645 - acc: 0.8472 - val_loss: 0.6002 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.60114 to 0.60016, saving model to ./params.hdf5\n",
      "Epoch 69/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5643 - acc: 0.8454 - val_loss: 0.5992 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.60016 to 0.59918, saving model to ./params.hdf5\n",
      "Epoch 70/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5617 - acc: 0.8463 - val_loss: 0.5984 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.59918 to 0.59837, saving model to ./params.hdf5\n",
      "Epoch 71/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5621 - acc: 0.8487 - val_loss: 0.5970 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.59837 to 0.59703, saving model to ./params.hdf5\n",
      "Epoch 72/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5587 - acc: 0.8481 - val_loss: 0.5960 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.59703 to 0.59595, saving model to ./params.hdf5\n",
      "Epoch 73/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5578 - acc: 0.8501 - val_loss: 0.5950 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.59595 to 0.59496, saving model to ./params.hdf5\n",
      "Epoch 74/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5563 - acc: 0.8518 - val_loss: 0.5939 - val_acc: 0.8513\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.59496 to 0.59387, saving model to ./params.hdf5\n",
      "Epoch 75/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5535 - acc: 0.8524 - val_loss: 0.5927 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.59387 to 0.59267, saving model to ./params.hdf5\n",
      "Epoch 76/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5534 - acc: 0.8529 - val_loss: 0.5918 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.59267 to 0.59182, saving model to ./params.hdf5\n",
      "Epoch 77/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5527 - acc: 0.8518 - val_loss: 0.5910 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.59182 to 0.59102, saving model to ./params.hdf5\n",
      "Epoch 78/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5517 - acc: 0.8516 - val_loss: 0.5900 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.59102 to 0.58995, saving model to ./params.hdf5\n",
      "Epoch 79/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5499 - acc: 0.8490 - val_loss: 0.5889 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.58995 to 0.58890, saving model to ./params.hdf5\n",
      "Epoch 80/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5491 - acc: 0.8493 - val_loss: 0.5878 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.58890 to 0.58785, saving model to ./params.hdf5\n",
      "Epoch 81/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5464 - acc: 0.8542 - val_loss: 0.5869 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.58785 to 0.58689, saving model to ./params.hdf5\n",
      "Epoch 82/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5458 - acc: 0.8528 - val_loss: 0.5860 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.58689 to 0.58600, saving model to ./params.hdf5\n",
      "Epoch 83/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5434 - acc: 0.8568 - val_loss: 0.5850 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.58600 to 0.58502, saving model to ./params.hdf5\n",
      "Epoch 84/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5425 - acc: 0.8564 - val_loss: 0.5841 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.58502 to 0.58411, saving model to ./params.hdf5\n",
      "Epoch 85/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5401 - acc: 0.8600 - val_loss: 0.5830 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.58411 to 0.58297, saving model to ./params.hdf5\n",
      "Epoch 86/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5401 - acc: 0.8564 - val_loss: 0.5821 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.58297 to 0.58210, saving model to ./params.hdf5\n",
      "Epoch 87/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5397 - acc: 0.8547 - val_loss: 0.5813 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.58210 to 0.58129, saving model to ./params.hdf5\n",
      "Epoch 88/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.5360 - acc: 0.8601 - val_loss: 0.5802 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.58129 to 0.58016, saving model to ./params.hdf5\n",
      "Epoch 89/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5354 - acc: 0.8592 - val_loss: 0.5795 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.58016 to 0.57953, saving model to ./params.hdf5\n",
      "Epoch 90/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5348 - acc: 0.8600 - val_loss: 0.5785 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.57953 to 0.57851, saving model to ./params.hdf5\n",
      "Epoch 91/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5332 - acc: 0.8586 - val_loss: 0.5775 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.57851 to 0.57747, saving model to ./params.hdf5\n",
      "Epoch 92/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5320 - acc: 0.8619 - val_loss: 0.5767 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.57747 to 0.57667, saving model to ./params.hdf5\n",
      "Epoch 93/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5308 - acc: 0.8618 - val_loss: 0.5757 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.57667 to 0.57569, saving model to ./params.hdf5\n",
      "Epoch 94/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5292 - acc: 0.8646 - val_loss: 0.5749 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.57569 to 0.57487, saving model to ./params.hdf5\n",
      "Epoch 95/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.5279 - acc: 0.8592 - val_loss: 0.5739 - val_acc: 0.8531\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.57487 to 0.57394, saving model to ./params.hdf5\n",
      "Epoch 96/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5277 - acc: 0.8638 - val_loss: 0.5732 - val_acc: 0.8521\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.57394 to 0.57322, saving model to ./params.hdf5\n",
      "Epoch 97/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5254 - acc: 0.8598 - val_loss: 0.5725 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.57322 to 0.57250, saving model to ./params.hdf5\n",
      "Epoch 98/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.5250 - acc: 0.8600 - val_loss: 0.5718 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.57250 to 0.57178, saving model to ./params.hdf5\n",
      "Epoch 99/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.5240 - acc: 0.8584 - val_loss: 0.5707 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.57178 to 0.57070, saving model to ./params.hdf5\n",
      "Epoch 100/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5220 - acc: 0.8627 - val_loss: 0.5699 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.57070 to 0.56993, saving model to ./params.hdf5\n",
      "Epoch 101/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5198 - acc: 0.8641 - val_loss: 0.5690 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.56993 to 0.56903, saving model to ./params.hdf5\n",
      "Epoch 102/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.5195 - acc: 0.8648 - val_loss: 0.5681 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.56903 to 0.56812, saving model to ./params.hdf5\n",
      "Epoch 103/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5183 - acc: 0.8606 - val_loss: 0.5673 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.56812 to 0.56726, saving model to ./params.hdf5\n",
      "Epoch 104/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.5161 - acc: 0.8652 - val_loss: 0.5666 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.56726 to 0.56662, saving model to ./params.hdf5\n",
      "Epoch 105/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5142 - acc: 0.8683 - val_loss: 0.5657 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.56662 to 0.56572, saving model to ./params.hdf5\n",
      "Epoch 106/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5147 - acc: 0.8643 - val_loss: 0.5647 - val_acc: 0.8521\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.56572 to 0.56473, saving model to ./params.hdf5\n",
      "Epoch 107/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5105 - acc: 0.8714 - val_loss: 0.5640 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.56473 to 0.56404, saving model to ./params.hdf5\n",
      "Epoch 108/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5118 - acc: 0.8647 - val_loss: 0.5632 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.56404 to 0.56317, saving model to ./params.hdf5\n",
      "Epoch 109/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.5108 - acc: 0.8663 - val_loss: 0.5624 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.56317 to 0.56238, saving model to ./params.hdf5\n",
      "Epoch 110/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5092 - acc: 0.8666 - val_loss: 0.5616 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.56238 to 0.56160, saving model to ./params.hdf5\n",
      "Epoch 111/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5086 - acc: 0.8672 - val_loss: 0.5607 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.56160 to 0.56074, saving model to ./params.hdf5\n",
      "Epoch 112/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5076 - acc: 0.8655 - val_loss: 0.5602 - val_acc: 0.8521\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.56074 to 0.56021, saving model to ./params.hdf5\n",
      "Epoch 113/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.5049 - acc: 0.8711 - val_loss: 0.5593 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.56021 to 0.55927, saving model to ./params.hdf5\n",
      "Epoch 114/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.5050 - acc: 0.8681 - val_loss: 0.5586 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.55927 to 0.55860, saving model to ./params.hdf5\n",
      "Epoch 115/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.5023 - acc: 0.8711 - val_loss: 0.5579 - val_acc: 0.8513\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.55860 to 0.55789, saving model to ./params.hdf5\n",
      "Epoch 116/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.5032 - acc: 0.8696 - val_loss: 0.5569 - val_acc: 0.8531\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.55789 to 0.55693, saving model to ./params.hdf5\n",
      "Epoch 117/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4997 - acc: 0.8728 - val_loss: 0.5562 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.55693 to 0.55617, saving model to ./params.hdf5\n",
      "Epoch 118/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4995 - acc: 0.8732 - val_loss: 0.5554 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.55617 to 0.55542, saving model to ./params.hdf5\n",
      "Epoch 119/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4989 - acc: 0.8704 - val_loss: 0.5546 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.55542 to 0.55459, saving model to ./params.hdf5\n",
      "Epoch 120/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4973 - acc: 0.8737 - val_loss: 0.5540 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.55459 to 0.55397, saving model to ./params.hdf5\n",
      "Epoch 121/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4950 - acc: 0.8748 - val_loss: 0.5530 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.55397 to 0.55304, saving model to ./params.hdf5\n",
      "Epoch 122/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4942 - acc: 0.8762 - val_loss: 0.5522 - val_acc: 0.8531\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.55304 to 0.55225, saving model to ./params.hdf5\n",
      "Epoch 123/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4942 - acc: 0.8745 - val_loss: 0.5515 - val_acc: 0.8513\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.55225 to 0.55152, saving model to ./params.hdf5\n",
      "Epoch 124/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4930 - acc: 0.8726 - val_loss: 0.5510 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.55152 to 0.55103, saving model to ./params.hdf5\n",
      "Epoch 125/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4910 - acc: 0.8773 - val_loss: 0.5503 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.55103 to 0.55027, saving model to ./params.hdf5\n",
      "Epoch 126/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4897 - acc: 0.8738 - val_loss: 0.5493 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.55027 to 0.54932, saving model to ./params.hdf5\n",
      "Epoch 127/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4895 - acc: 0.8742 - val_loss: 0.5488 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.54932 to 0.54881, saving model to ./params.hdf5\n",
      "Epoch 128/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4884 - acc: 0.8748 - val_loss: 0.5479 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.54881 to 0.54793, saving model to ./params.hdf5\n",
      "Epoch 129/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4873 - acc: 0.8774 - val_loss: 0.5471 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.54793 to 0.54709, saving model to ./params.hdf5\n",
      "Epoch 130/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.4859 - acc: 0.8773 - val_loss: 0.5464 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.54709 to 0.54644, saving model to ./params.hdf5\n",
      "Epoch 131/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.4854 - acc: 0.8762 - val_loss: 0.5455 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.54644 to 0.54551, saving model to ./params.hdf5\n",
      "Epoch 132/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4841 - acc: 0.8775 - val_loss: 0.5448 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.54551 to 0.54481, saving model to ./params.hdf5\n",
      "Epoch 133/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4828 - acc: 0.8763 - val_loss: 0.5441 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.54481 to 0.54412, saving model to ./params.hdf5\n",
      "Epoch 134/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4796 - acc: 0.8801 - val_loss: 0.5434 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.54412 to 0.54345, saving model to ./params.hdf5\n",
      "Epoch 135/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4819 - acc: 0.8764 - val_loss: 0.5429 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.54345 to 0.54286, saving model to ./params.hdf5\n",
      "Epoch 136/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4803 - acc: 0.8796 - val_loss: 0.5420 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.54286 to 0.54205, saving model to ./params.hdf5\n",
      "Epoch 137/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4765 - acc: 0.8825 - val_loss: 0.5414 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.54205 to 0.54140, saving model to ./params.hdf5\n",
      "Epoch 138/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4773 - acc: 0.8780 - val_loss: 0.5407 - val_acc: 0.8513\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.54140 to 0.54069, saving model to ./params.hdf5\n",
      "Epoch 139/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4739 - acc: 0.8829 - val_loss: 0.5402 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.54069 to 0.54016, saving model to ./params.hdf5\n",
      "Epoch 140/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4733 - acc: 0.8831 - val_loss: 0.5394 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.54016 to 0.53943, saving model to ./params.hdf5\n",
      "Epoch 141/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4739 - acc: 0.8805 - val_loss: 0.5386 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.53943 to 0.53863, saving model to ./params.hdf5\n",
      "Epoch 142/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4716 - acc: 0.8782 - val_loss: 0.5379 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.53863 to 0.53793, saving model to ./params.hdf5\n",
      "Epoch 143/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.4703 - acc: 0.8825 - val_loss: 0.5374 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.53793 to 0.53743, saving model to ./params.hdf5\n",
      "Epoch 144/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4703 - acc: 0.8785 - val_loss: 0.5367 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.53743 to 0.53671, saving model to ./params.hdf5\n",
      "Epoch 145/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4684 - acc: 0.8840 - val_loss: 0.5362 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.53671 to 0.53620, saving model to ./params.hdf5\n",
      "Epoch 146/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4685 - acc: 0.8829 - val_loss: 0.5352 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.53620 to 0.53522, saving model to ./params.hdf5\n",
      "Epoch 147/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.4686 - acc: 0.8811 - val_loss: 0.5346 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.53522 to 0.53464, saving model to ./params.hdf5\n",
      "Epoch 148/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.4663 - acc: 0.8824 - val_loss: 0.5340 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.53464 to 0.53398, saving model to ./params.hdf5\n",
      "Epoch 149/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4640 - acc: 0.8839 - val_loss: 0.5333 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.53398 to 0.53334, saving model to ./params.hdf5\n",
      "Epoch 150/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4635 - acc: 0.8838 - val_loss: 0.5326 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.53334 to 0.53262, saving model to ./params.hdf5\n",
      "Epoch 151/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4633 - acc: 0.8812 - val_loss: 0.5319 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.53262 to 0.53192, saving model to ./params.hdf5\n",
      "Epoch 152/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4599 - acc: 0.8860 - val_loss: 0.5313 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.53192 to 0.53127, saving model to ./params.hdf5\n",
      "Epoch 153/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4599 - acc: 0.8858 - val_loss: 0.5305 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.53127 to 0.53048, saving model to ./params.hdf5\n",
      "Epoch 154/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4579 - acc: 0.8856 - val_loss: 0.5299 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.53048 to 0.52989, saving model to ./params.hdf5\n",
      "Epoch 155/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4594 - acc: 0.8810 - val_loss: 0.5293 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.52989 to 0.52927, saving model to ./params.hdf5\n",
      "Epoch 156/300\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.4577 - acc: 0.8858 - val_loss: 0.5288 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.52927 to 0.52882, saving model to ./params.hdf5\n",
      "Epoch 157/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4572 - acc: 0.8856 - val_loss: 0.5283 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.52882 to 0.52826, saving model to ./params.hdf5\n",
      "Epoch 158/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4564 - acc: 0.8859 - val_loss: 0.5276 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.52826 to 0.52760, saving model to ./params.hdf5\n",
      "Epoch 159/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4539 - acc: 0.8874 - val_loss: 0.5270 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.52760 to 0.52700, saving model to ./params.hdf5\n",
      "Epoch 160/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4534 - acc: 0.8853 - val_loss: 0.5262 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.52700 to 0.52620, saving model to ./params.hdf5\n",
      "Epoch 161/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4523 - acc: 0.8861 - val_loss: 0.5257 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.52620 to 0.52575, saving model to ./params.hdf5\n",
      "Epoch 162/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4512 - acc: 0.8871 - val_loss: 0.5251 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.52575 to 0.52507, saving model to ./params.hdf5\n",
      "Epoch 163/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4515 - acc: 0.8861 - val_loss: 0.5245 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.52507 to 0.52446, saving model to ./params.hdf5\n",
      "Epoch 164/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.4485 - acc: 0.8878 - val_loss: 0.5240 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.52446 to 0.52398, saving model to ./params.hdf5\n",
      "Epoch 165/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4479 - acc: 0.8898 - val_loss: 0.5232 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.52398 to 0.52321, saving model to ./params.hdf5\n",
      "Epoch 166/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4478 - acc: 0.8892 - val_loss: 0.5227 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.52321 to 0.52266, saving model to ./params.hdf5\n",
      "Epoch 167/300\n",
      "19852/19852 [==============================] - 13s 643us/step - loss: 0.4466 - acc: 0.8887 - val_loss: 0.5221 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.52266 to 0.52211, saving model to ./params.hdf5\n",
      "Epoch 168/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4455 - acc: 0.8876 - val_loss: 0.5213 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.52211 to 0.52126, saving model to ./params.hdf5\n",
      "Epoch 169/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4419 - acc: 0.8928 - val_loss: 0.5206 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.52126 to 0.52064, saving model to ./params.hdf5\n",
      "Epoch 170/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4446 - acc: 0.8885 - val_loss: 0.5202 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.52064 to 0.52017, saving model to ./params.hdf5\n",
      "Epoch 171/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4433 - acc: 0.8867 - val_loss: 0.5196 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.52017 to 0.51958, saving model to ./params.hdf5\n",
      "Epoch 172/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4394 - acc: 0.8927 - val_loss: 0.5189 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.51958 to 0.51890, saving model to ./params.hdf5\n",
      "Epoch 173/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4397 - acc: 0.8918 - val_loss: 0.5183 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.51890 to 0.51829, saving model to ./params.hdf5\n",
      "Epoch 174/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4404 - acc: 0.8872 - val_loss: 0.5178 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.51829 to 0.51782, saving model to ./params.hdf5\n",
      "Epoch 175/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4381 - acc: 0.8916 - val_loss: 0.5171 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.51782 to 0.51711, saving model to ./params.hdf5\n",
      "Epoch 176/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4394 - acc: 0.8893 - val_loss: 0.5164 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.51711 to 0.51645, saving model to ./params.hdf5\n",
      "Epoch 177/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.4328 - acc: 0.8952 - val_loss: 0.5159 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.51645 to 0.51594, saving model to ./params.hdf5\n",
      "Epoch 178/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4361 - acc: 0.8914 - val_loss: 0.5153 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.51594 to 0.51525, saving model to ./params.hdf5\n",
      "Epoch 179/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.4328 - acc: 0.8950 - val_loss: 0.5146 - val_acc: 0.8463\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.51525 to 0.51464, saving model to ./params.hdf5\n",
      "Epoch 180/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4342 - acc: 0.8909 - val_loss: 0.5140 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.51464 to 0.51397, saving model to ./params.hdf5\n",
      "Epoch 181/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4330 - acc: 0.8909 - val_loss: 0.5135 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.51397 to 0.51346, saving model to ./params.hdf5\n",
      "Epoch 182/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4303 - acc: 0.8953 - val_loss: 0.5129 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.51346 to 0.51294, saving model to ./params.hdf5\n",
      "Epoch 183/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4281 - acc: 0.8962 - val_loss: 0.5125 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.51294 to 0.51251, saving model to ./params.hdf5\n",
      "Epoch 184/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4293 - acc: 0.8952 - val_loss: 0.5119 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.51251 to 0.51187, saving model to ./params.hdf5\n",
      "Epoch 185/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4280 - acc: 0.8964 - val_loss: 0.5112 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.51187 to 0.51117, saving model to ./params.hdf5\n",
      "Epoch 186/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4276 - acc: 0.8952 - val_loss: 0.5108 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.51117 to 0.51081, saving model to ./params.hdf5\n",
      "Epoch 187/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4266 - acc: 0.8941 - val_loss: 0.5101 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.51081 to 0.51009, saving model to ./params.hdf5\n",
      "Epoch 188/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4256 - acc: 0.8943 - val_loss: 0.5095 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.51009 to 0.50946, saving model to ./params.hdf5\n",
      "Epoch 189/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4247 - acc: 0.8969 - val_loss: 0.5087 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.50946 to 0.50875, saving model to ./params.hdf5\n",
      "Epoch 190/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4219 - acc: 0.8973 - val_loss: 0.5081 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.50875 to 0.50812, saving model to ./params.hdf5\n",
      "Epoch 191/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4221 - acc: 0.8976 - val_loss: 0.5075 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.50812 to 0.50750, saving model to ./params.hdf5\n",
      "Epoch 192/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.4216 - acc: 0.8970 - val_loss: 0.5071 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.50750 to 0.50714, saving model to ./params.hdf5\n",
      "Epoch 193/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4202 - acc: 0.8960 - val_loss: 0.5067 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.50714 to 0.50670, saving model to ./params.hdf5\n",
      "Epoch 194/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4180 - acc: 0.8991 - val_loss: 0.5061 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.50670 to 0.50605, saving model to ./params.hdf5\n",
      "Epoch 195/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.4185 - acc: 0.8982 - val_loss: 0.5056 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.50605 to 0.50560, saving model to ./params.hdf5\n",
      "Epoch 196/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4192 - acc: 0.9015 - val_loss: 0.5050 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.50560 to 0.50500, saving model to ./params.hdf5\n",
      "Epoch 197/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4191 - acc: 0.8957 - val_loss: 0.5045 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.50500 to 0.50447, saving model to ./params.hdf5\n",
      "Epoch 198/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4165 - acc: 0.8979 - val_loss: 0.5039 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.50447 to 0.50387, saving model to ./params.hdf5\n",
      "Epoch 199/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.4164 - acc: 0.9008 - val_loss: 0.5036 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.50387 to 0.50363, saving model to ./params.hdf5\n",
      "Epoch 200/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.4154 - acc: 0.8957 - val_loss: 0.5029 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.50363 to 0.50288, saving model to ./params.hdf5\n",
      "Epoch 201/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4131 - acc: 0.8982 - val_loss: 0.5024 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.50288 to 0.50236, saving model to ./params.hdf5\n",
      "Epoch 202/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4151 - acc: 0.8963 - val_loss: 0.5018 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.50236 to 0.50181, saving model to ./params.hdf5\n",
      "Epoch 203/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4122 - acc: 0.8990 - val_loss: 0.5013 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.50181 to 0.50128, saving model to ./params.hdf5\n",
      "Epoch 204/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4100 - acc: 0.9020 - val_loss: 0.5005 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.50128 to 0.50054, saving model to ./params.hdf5\n",
      "Epoch 205/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4108 - acc: 0.9023 - val_loss: 0.4999 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.50054 to 0.49988, saving model to ./params.hdf5\n",
      "Epoch 206/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.4094 - acc: 0.9010 - val_loss: 0.4995 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.49988 to 0.49947, saving model to ./params.hdf5\n",
      "Epoch 207/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.4080 - acc: 0.9004 - val_loss: 0.4991 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.49947 to 0.49906, saving model to ./params.hdf5\n",
      "Epoch 208/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4081 - acc: 0.9020 - val_loss: 0.4987 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.49906 to 0.49869, saving model to ./params.hdf5\n",
      "Epoch 209/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4080 - acc: 0.9001 - val_loss: 0.4982 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.49869 to 0.49819, saving model to ./params.hdf5\n",
      "Epoch 210/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4051 - acc: 0.9018 - val_loss: 0.4977 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.49819 to 0.49774, saving model to ./params.hdf5\n",
      "Epoch 211/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.4064 - acc: 0.8984 - val_loss: 0.4974 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.49774 to 0.49741, saving model to ./params.hdf5\n",
      "Epoch 212/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4047 - acc: 0.9028 - val_loss: 0.4970 - val_acc: 0.8459\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.49741 to 0.49703, saving model to ./params.hdf5\n",
      "Epoch 213/300\n",
      "19852/19852 [==============================] - 13s 651us/step - loss: 0.4044 - acc: 0.8998 - val_loss: 0.4961 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.49703 to 0.49612, saving model to ./params.hdf5\n",
      "Epoch 214/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.4035 - acc: 0.9035 - val_loss: 0.4957 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.49612 to 0.49571, saving model to ./params.hdf5\n",
      "Epoch 215/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.4005 - acc: 0.9045 - val_loss: 0.4956 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.49571 to 0.49555, saving model to ./params.hdf5\n",
      "Epoch 216/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3998 - acc: 0.9057 - val_loss: 0.4949 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.49555 to 0.49487, saving model to ./params.hdf5\n",
      "Epoch 217/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3984 - acc: 0.9041 - val_loss: 0.4943 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.49487 to 0.49435, saving model to ./params.hdf5\n",
      "Epoch 218/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3996 - acc: 0.9045 - val_loss: 0.4939 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.49435 to 0.49385, saving model to ./params.hdf5\n",
      "Epoch 219/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3977 - acc: 0.9037 - val_loss: 0.4934 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.49385 to 0.49341, saving model to ./params.hdf5\n",
      "Epoch 220/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3974 - acc: 0.9044 - val_loss: 0.4929 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.49341 to 0.49291, saving model to ./params.hdf5\n",
      "Epoch 221/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.3987 - acc: 0.9036 - val_loss: 0.4924 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.49291 to 0.49237, saving model to ./params.hdf5\n",
      "Epoch 222/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3947 - acc: 0.9037 - val_loss: 0.4921 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.49237 to 0.49206, saving model to ./params.hdf5\n",
      "Epoch 223/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3948 - acc: 0.9047 - val_loss: 0.4914 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.49206 to 0.49141, saving model to ./params.hdf5\n",
      "Epoch 224/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3949 - acc: 0.9018 - val_loss: 0.4908 - val_acc: 0.8469\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.49141 to 0.49083, saving model to ./params.hdf5\n",
      "Epoch 225/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3917 - acc: 0.9056 - val_loss: 0.4906 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.49083 to 0.49061, saving model to ./params.hdf5\n",
      "Epoch 226/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3922 - acc: 0.9050 - val_loss: 0.4900 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.49061 to 0.49003, saving model to ./params.hdf5\n",
      "Epoch 227/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3893 - acc: 0.9096 - val_loss: 0.4895 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.49003 to 0.48949, saving model to ./params.hdf5\n",
      "Epoch 228/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3887 - acc: 0.9107 - val_loss: 0.4888 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.48949 to 0.48882, saving model to ./params.hdf5\n",
      "Epoch 229/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3903 - acc: 0.9053 - val_loss: 0.4884 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.48882 to 0.48844, saving model to ./params.hdf5\n",
      "Epoch 230/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3884 - acc: 0.9083 - val_loss: 0.4881 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.48844 to 0.48811, saving model to ./params.hdf5\n",
      "Epoch 231/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3884 - acc: 0.9050 - val_loss: 0.4876 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.48811 to 0.48759, saving model to ./params.hdf5\n",
      "Epoch 232/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3863 - acc: 0.9082 - val_loss: 0.4869 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.48759 to 0.48692, saving model to ./params.hdf5\n",
      "Epoch 233/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3871 - acc: 0.9090 - val_loss: 0.4864 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.48692 to 0.48643, saving model to ./params.hdf5\n",
      "Epoch 234/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3839 - acc: 0.9098 - val_loss: 0.4860 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.48643 to 0.48597, saving model to ./params.hdf5\n",
      "Epoch 235/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3865 - acc: 0.9064 - val_loss: 0.4857 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.48597 to 0.48568, saving model to ./params.hdf5\n",
      "Epoch 236/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3863 - acc: 0.9071 - val_loss: 0.4849 - val_acc: 0.8469\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.48568 to 0.48488, saving model to ./params.hdf5\n",
      "Epoch 237/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.3839 - acc: 0.9103 - val_loss: 0.4845 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.48488 to 0.48452, saving model to ./params.hdf5\n",
      "Epoch 238/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3825 - acc: 0.9072 - val_loss: 0.4841 - val_acc: 0.8469\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.48452 to 0.48415, saving model to ./params.hdf5\n",
      "Epoch 239/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3818 - acc: 0.9101 - val_loss: 0.4838 - val_acc: 0.8469\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.48415 to 0.48381, saving model to ./params.hdf5\n",
      "Epoch 240/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.3807 - acc: 0.9083 - val_loss: 0.4835 - val_acc: 0.8463\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.48381 to 0.48346, saving model to ./params.hdf5\n",
      "Epoch 241/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.3803 - acc: 0.9080 - val_loss: 0.4827 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.48346 to 0.48272, saving model to ./params.hdf5\n",
      "Epoch 242/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3788 - acc: 0.9084 - val_loss: 0.4822 - val_acc: 0.8461\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.48272 to 0.48217, saving model to ./params.hdf5\n",
      "Epoch 243/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3782 - acc: 0.9094 - val_loss: 0.4820 - val_acc: 0.8457\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.48217 to 0.48198, saving model to ./params.hdf5\n",
      "Epoch 244/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3769 - acc: 0.9120 - val_loss: 0.4815 - val_acc: 0.8461\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.48198 to 0.48152, saving model to ./params.hdf5\n",
      "Epoch 245/300\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.3780 - acc: 0.9102 - val_loss: 0.4813 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.48152 to 0.48130, saving model to ./params.hdf5\n",
      "Epoch 246/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3757 - acc: 0.9133 - val_loss: 0.4808 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.48130 to 0.48079, saving model to ./params.hdf5\n",
      "Epoch 247/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3756 - acc: 0.9120 - val_loss: 0.4800 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.48079 to 0.48003, saving model to ./params.hdf5\n",
      "Epoch 248/300\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.3749 - acc: 0.9110 - val_loss: 0.4799 - val_acc: 0.8469\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.48003 to 0.47985, saving model to ./params.hdf5\n",
      "Epoch 249/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3722 - acc: 0.9120 - val_loss: 0.4793 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.47985 to 0.47934, saving model to ./params.hdf5\n",
      "Epoch 250/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3743 - acc: 0.9088 - val_loss: 0.4788 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.47934 to 0.47883, saving model to ./params.hdf5\n",
      "Epoch 251/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3724 - acc: 0.9111 - val_loss: 0.4786 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.47883 to 0.47858, saving model to ./params.hdf5\n",
      "Epoch 252/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3732 - acc: 0.9074 - val_loss: 0.4781 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.47858 to 0.47805, saving model to ./params.hdf5\n",
      "Epoch 253/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3708 - acc: 0.9107 - val_loss: 0.4778 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.47805 to 0.47778, saving model to ./params.hdf5\n",
      "Epoch 254/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3691 - acc: 0.9110 - val_loss: 0.4772 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.47778 to 0.47716, saving model to ./params.hdf5\n",
      "Epoch 255/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3693 - acc: 0.9125 - val_loss: 0.4765 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.47716 to 0.47654, saving model to ./params.hdf5\n",
      "Epoch 256/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.3695 - acc: 0.9126 - val_loss: 0.4761 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.47654 to 0.47613, saving model to ./params.hdf5\n",
      "Epoch 257/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3670 - acc: 0.9138 - val_loss: 0.4760 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.47613 to 0.47598, saving model to ./params.hdf5\n",
      "Epoch 258/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3651 - acc: 0.9149 - val_loss: 0.4755 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.47598 to 0.47545, saving model to ./params.hdf5\n",
      "Epoch 259/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3643 - acc: 0.9175 - val_loss: 0.4750 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.47545 to 0.47504, saving model to ./params.hdf5\n",
      "Epoch 260/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3653 - acc: 0.9166 - val_loss: 0.4745 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.47504 to 0.47454, saving model to ./params.hdf5\n",
      "Epoch 261/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3632 - acc: 0.9162 - val_loss: 0.4742 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.47454 to 0.47422, saving model to ./params.hdf5\n",
      "Epoch 262/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3640 - acc: 0.9130 - val_loss: 0.4738 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.47422 to 0.47375, saving model to ./params.hdf5\n",
      "Epoch 263/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.3629 - acc: 0.9133 - val_loss: 0.4730 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.47375 to 0.47300, saving model to ./params.hdf5\n",
      "Epoch 264/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.3632 - acc: 0.9156 - val_loss: 0.4729 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.47300 to 0.47287, saving model to ./params.hdf5\n",
      "Epoch 265/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3629 - acc: 0.9137 - val_loss: 0.4725 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.47287 to 0.47246, saving model to ./params.hdf5\n",
      "Epoch 266/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.3592 - acc: 0.9154 - val_loss: 0.4719 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.47246 to 0.47189, saving model to ./params.hdf5\n",
      "Epoch 267/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.3619 - acc: 0.9129 - val_loss: 0.4718 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.47189 to 0.47178, saving model to ./params.hdf5\n",
      "Epoch 268/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3622 - acc: 0.9140 - val_loss: 0.4714 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.47178 to 0.47140, saving model to ./params.hdf5\n",
      "Epoch 269/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3574 - acc: 0.9174 - val_loss: 0.4711 - val_acc: 0.8469\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.47140 to 0.47113, saving model to ./params.hdf5\n",
      "Epoch 270/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3570 - acc: 0.9165 - val_loss: 0.4708 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.47113 to 0.47078, saving model to ./params.hdf5\n",
      "Epoch 271/300\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.3559 - acc: 0.9183 - val_loss: 0.4706 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.47078 to 0.47060, saving model to ./params.hdf5\n",
      "Epoch 272/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3534 - acc: 0.9212 - val_loss: 0.4700 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.47060 to 0.47004, saving model to ./params.hdf5\n",
      "Epoch 273/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3549 - acc: 0.9160 - val_loss: 0.4696 - val_acc: 0.8453\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.47004 to 0.46964, saving model to ./params.hdf5\n",
      "Epoch 274/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3546 - acc: 0.9185 - val_loss: 0.4696 - val_acc: 0.8463\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.46964 to 0.46964, saving model to ./params.hdf5\n",
      "Epoch 275/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3536 - acc: 0.9193 - val_loss: 0.4689 - val_acc: 0.8469\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.46964 to 0.46890, saving model to ./params.hdf5\n",
      "Epoch 276/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3523 - acc: 0.9225 - val_loss: 0.4688 - val_acc: 0.8459\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.46890 to 0.46877, saving model to ./params.hdf5\n",
      "Epoch 277/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3548 - acc: 0.9152 - val_loss: 0.4680 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.46877 to 0.46800, saving model to ./params.hdf5\n",
      "Epoch 278/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3498 - acc: 0.9219 - val_loss: 0.4674 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.46800 to 0.46739, saving model to ./params.hdf5\n",
      "Epoch 279/300\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.3506 - acc: 0.9194 - val_loss: 0.4673 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.46739 to 0.46733, saving model to ./params.hdf5\n",
      "Epoch 280/300\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.3494 - acc: 0.9205 - val_loss: 0.4670 - val_acc: 0.8453\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.46733 to 0.46702, saving model to ./params.hdf5\n",
      "Epoch 281/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3487 - acc: 0.9235 - val_loss: 0.4664 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.46702 to 0.46642, saving model to ./params.hdf5\n",
      "Epoch 282/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.3476 - acc: 0.9232 - val_loss: 0.4660 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.46642 to 0.46595, saving model to ./params.hdf5\n",
      "Epoch 283/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3460 - acc: 0.9209 - val_loss: 0.4654 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.46595 to 0.46537, saving model to ./params.hdf5\n",
      "Epoch 284/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3479 - acc: 0.9206 - val_loss: 0.4649 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.46537 to 0.46486, saving model to ./params.hdf5\n",
      "Epoch 285/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3450 - acc: 0.9235 - val_loss: 0.4646 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.46486 to 0.46465, saving model to ./params.hdf5\n",
      "Epoch 286/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3451 - acc: 0.9223 - val_loss: 0.4647 - val_acc: 0.8455\n",
      "\n",
      "Epoch 00286: val_loss did not improve\n",
      "Epoch 287/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3433 - acc: 0.9229 - val_loss: 0.4645 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.46465 to 0.46446, saving model to ./params.hdf5\n",
      "Epoch 288/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3435 - acc: 0.9223 - val_loss: 0.4632 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.46446 to 0.46317, saving model to ./params.hdf5\n",
      "Epoch 289/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3434 - acc: 0.9231 - val_loss: 0.4631 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.46317 to 0.46310, saving model to ./params.hdf5\n",
      "Epoch 290/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3419 - acc: 0.9221 - val_loss: 0.4626 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.46310 to 0.46259, saving model to ./params.hdf5\n",
      "Epoch 291/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3419 - acc: 0.9230 - val_loss: 0.4623 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.46259 to 0.46231, saving model to ./params.hdf5\n",
      "Epoch 292/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3412 - acc: 0.9221 - val_loss: 0.4618 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.46231 to 0.46184, saving model to ./params.hdf5\n",
      "Epoch 293/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3383 - acc: 0.9245 - val_loss: 0.4616 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.46184 to 0.46158, saving model to ./params.hdf5\n",
      "Epoch 294/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3407 - acc: 0.9190 - val_loss: 0.4613 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.46158 to 0.46129, saving model to ./params.hdf5\n",
      "Epoch 295/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3384 - acc: 0.9233 - val_loss: 0.4607 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.46129 to 0.46066, saving model to ./params.hdf5\n",
      "Epoch 296/300\n",
      "19852/19852 [==============================] - 13s 646us/step - loss: 0.3408 - acc: 0.9187 - val_loss: 0.4607 - val_acc: 0.8463\n",
      "\n",
      "Epoch 00296: val_loss did not improve\n",
      "Epoch 297/300\n",
      "19852/19852 [==============================] - 13s 648us/step - loss: 0.3350 - acc: 0.9253 - val_loss: 0.4605 - val_acc: 0.8455\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.46066 to 0.46050, saving model to ./params.hdf5\n",
      "Epoch 298/300\n",
      "19852/19852 [==============================] - 13s 645us/step - loss: 0.3344 - acc: 0.9256 - val_loss: 0.4597 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.46050 to 0.45970, saving model to ./params.hdf5\n",
      "Epoch 299/300\n",
      "19852/19852 [==============================] - 13s 644us/step - loss: 0.3345 - acc: 0.9251 - val_loss: 0.4596 - val_acc: 0.8457\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.45970 to 0.45962, saving model to ./params.hdf5\n",
      "Epoch 300/300\n",
      "19852/19852 [==============================] - 13s 647us/step - loss: 0.3359 - acc: 0.9237 - val_loss: 0.4588 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.45962 to 0.45877, saving model to ./params.hdf5\n"
     ]
    }
   ],
   "source": [
    "history = model_avg.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=300 #epochs\n",
    "                        ,validation_split=0.2, shuffle=True, callbacks=[save_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19852 samples, validate on 4964 samples\n",
      "Epoch 301/500\n",
      "19852/19852 [==============================] - 13s 657us/step - loss: 0.2602 - acc: 0.9694 - val_loss: 0.4438 - val_acc: 0.8445\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.44410 to 0.44375, saving model to ./params.hdf5\n",
      "Epoch 302/500\n",
      "19852/19852 [==============================] - 13s 654us/step - loss: 0.2601 - acc: 0.9702 - val_loss: 0.4433 - val_acc: 0.8433\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.44375 to 0.44335, saving model to ./params.hdf5\n",
      "Epoch 303/500\n",
      "19852/19852 [==============================] - 13s 656us/step - loss: 0.2606 - acc: 0.9682 - val_loss: 0.4430 - val_acc: 0.8443\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.44335 to 0.44302, saving model to ./params.hdf5\n",
      "Epoch 304/500\n",
      "19852/19852 [==============================] - 13s 654us/step - loss: 0.2594 - acc: 0.9694 - val_loss: 0.4427 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.44302 to 0.44266, saving model to ./params.hdf5\n",
      "Epoch 305/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2605 - acc: 0.9686 - val_loss: 0.4423 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00305: val_loss improved from 0.44266 to 0.44235, saving model to ./params.hdf5\n",
      "Epoch 306/500\n",
      "19852/19852 [==============================] - 13s 654us/step - loss: 0.2574 - acc: 0.9699 - val_loss: 0.4419 - val_acc: 0.8449\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.44235 to 0.44192, saving model to ./params.hdf5\n",
      "Epoch 307/500\n",
      "19852/19852 [==============================] - 13s 657us/step - loss: 0.2573 - acc: 0.9684 - val_loss: 0.4416 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.44192 to 0.44164, saving model to ./params.hdf5\n",
      "Epoch 308/500\n",
      "19852/19852 [==============================] - 13s 655us/step - loss: 0.2566 - acc: 0.9696 - val_loss: 0.4413 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.44164 to 0.44132, saving model to ./params.hdf5\n",
      "Epoch 309/500\n",
      "19852/19852 [==============================] - 13s 653us/step - loss: 0.2558 - acc: 0.9703 - val_loss: 0.4410 - val_acc: 0.8443\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.44132 to 0.44099, saving model to ./params.hdf5\n",
      "Epoch 310/500\n",
      "19852/19852 [==============================] - 13s 657us/step - loss: 0.2553 - acc: 0.9692 - val_loss: 0.4408 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.44099 to 0.44082, saving model to ./params.hdf5\n",
      "Epoch 311/500\n",
      "19852/19852 [==============================] - 13s 655us/step - loss: 0.2551 - acc: 0.9705 - val_loss: 0.4404 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.44082 to 0.44042, saving model to ./params.hdf5\n",
      "Epoch 312/500\n",
      "19852/19852 [==============================] - 13s 656us/step - loss: 0.2537 - acc: 0.9693 - val_loss: 0.4401 - val_acc: 0.8433\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.44042 to 0.44006, saving model to ./params.hdf5\n",
      "Epoch 313/500\n",
      "19852/19852 [==============================] - 13s 656us/step - loss: 0.2534 - acc: 0.9706 - val_loss: 0.4397 - val_acc: 0.8433\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.44006 to 0.43967, saving model to ./params.hdf5\n",
      "Epoch 314/500\n",
      "19852/19852 [==============================] - 13s 655us/step - loss: 0.2529 - acc: 0.9693 - val_loss: 0.4396 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.43967 to 0.43961, saving model to ./params.hdf5\n",
      "Epoch 315/500\n",
      "19852/19852 [==============================] - 13s 653us/step - loss: 0.2515 - acc: 0.9696 - val_loss: 0.4391 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.43961 to 0.43914, saving model to ./params.hdf5\n",
      "Epoch 316/500\n",
      "19852/19852 [==============================] - 13s 653us/step - loss: 0.2512 - acc: 0.9692 - val_loss: 0.4392 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00316: val_loss did not improve\n",
      "Epoch 317/500\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.2505 - acc: 0.9690 - val_loss: 0.4387 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.43914 to 0.43866, saving model to ./params.hdf5\n",
      "Epoch 318/500\n",
      "19852/19852 [==============================] - 13s 653us/step - loss: 0.2496 - acc: 0.9678 - val_loss: 0.4382 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.43866 to 0.43816, saving model to ./params.hdf5\n",
      "Epoch 319/500\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.2497 - acc: 0.9691 - val_loss: 0.4376 - val_acc: 0.8433\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.43816 to 0.43764, saving model to ./params.hdf5\n",
      "Epoch 320/500\n",
      "19852/19852 [==============================] - 13s 653us/step - loss: 0.2482 - acc: 0.9711 - val_loss: 0.4374 - val_acc: 0.8445\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.43764 to 0.43739, saving model to ./params.hdf5\n",
      "Epoch 321/500\n",
      "19852/19852 [==============================] - 13s 654us/step - loss: 0.2478 - acc: 0.9713 - val_loss: 0.4372 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.43739 to 0.43717, saving model to ./params.hdf5\n",
      "Epoch 322/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2463 - acc: 0.9698 - val_loss: 0.4371 - val_acc: 0.8427\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.43717 to 0.43705, saving model to ./params.hdf5\n",
      "Epoch 323/500\n",
      "19852/19852 [==============================] - 13s 654us/step - loss: 0.2464 - acc: 0.9700 - val_loss: 0.4366 - val_acc: 0.8447\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.43705 to 0.43662, saving model to ./params.hdf5\n",
      "Epoch 324/500\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.2451 - acc: 0.9709 - val_loss: 0.4362 - val_acc: 0.8423\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.43662 to 0.43618, saving model to ./params.hdf5\n",
      "Epoch 325/500\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.2453 - acc: 0.9707 - val_loss: 0.4359 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.43618 to 0.43594, saving model to ./params.hdf5\n",
      "Epoch 326/500\n",
      "19852/19852 [==============================] - 13s 653us/step - loss: 0.2450 - acc: 0.9711 - val_loss: 0.4357 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.43594 to 0.43567, saving model to ./params.hdf5\n",
      "Epoch 327/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2443 - acc: 0.9712 - val_loss: 0.4354 - val_acc: 0.8419\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.43567 to 0.43542, saving model to ./params.hdf5\n",
      "Epoch 328/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2425 - acc: 0.9710 - val_loss: 0.4352 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.43542 to 0.43523, saving model to ./params.hdf5\n",
      "Epoch 329/500\n",
      "19852/19852 [==============================] - 13s 651us/step - loss: 0.2414 - acc: 0.9719 - val_loss: 0.4351 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.43523 to 0.43507, saving model to ./params.hdf5\n",
      "Epoch 330/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2409 - acc: 0.9712 - val_loss: 0.4352 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00330: val_loss did not improve\n",
      "Epoch 331/500\n",
      "19852/19852 [==============================] - 13s 653us/step - loss: 0.2399 - acc: 0.9703 - val_loss: 0.4348 - val_acc: 0.8409\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.43507 to 0.43480, saving model to ./params.hdf5\n",
      "Epoch 332/500\n",
      "19852/19852 [==============================] - 13s 653us/step - loss: 0.2405 - acc: 0.9720 - val_loss: 0.4344 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00332: val_loss improved from 0.43480 to 0.43440, saving model to ./params.hdf5\n",
      "Epoch 333/500\n",
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.2396 - acc: 0.9714 - val_loss: 0.4338 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.43440 to 0.43378, saving model to ./params.hdf5\n",
      "Epoch 334/500\n",
      "19852/19852 [==============================] - 13s 651us/step - loss: 0.2397 - acc: 0.9717 - val_loss: 0.4338 - val_acc: 0.8423\n",
      "\n",
      "Epoch 00334: val_loss did not improve\n",
      "Epoch 335/500\n",
      "19852/19852 [==============================] - 13s 655us/step - loss: 0.2384 - acc: 0.9718 - val_loss: 0.4336 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.43378 to 0.43362, saving model to ./params.hdf5\n",
      "Epoch 336/500\n",
      "19852/19852 [==============================] - 13s 651us/step - loss: 0.2387 - acc: 0.9710 - val_loss: 0.4332 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.43362 to 0.43322, saving model to ./params.hdf5\n",
      "Epoch 337/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19852/19852 [==============================] - 13s 649us/step - loss: 0.2363 - acc: 0.9720 - val_loss: 0.4327 - val_acc: 0.8443\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.43322 to 0.43271, saving model to ./params.hdf5\n",
      "Epoch 338/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2372 - acc: 0.9722 - val_loss: 0.4327 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.43271 to 0.43268, saving model to ./params.hdf5\n",
      "Epoch 339/500\n",
      "19852/19852 [==============================] - 13s 654us/step - loss: 0.2361 - acc: 0.9714 - val_loss: 0.4327 - val_acc: 0.8409\n",
      "\n",
      "Epoch 00339: val_loss did not improve\n",
      "Epoch 340/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2356 - acc: 0.9722 - val_loss: 0.4321 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.43268 to 0.43205, saving model to ./params.hdf5\n",
      "Epoch 341/500\n",
      "19852/19852 [==============================] - 13s 653us/step - loss: 0.2341 - acc: 0.9712 - val_loss: 0.4318 - val_acc: 0.8433\n",
      "\n",
      "Epoch 00341: val_loss improved from 0.43205 to 0.43178, saving model to ./params.hdf5\n",
      "Epoch 342/500\n",
      "19852/19852 [==============================] - 13s 653us/step - loss: 0.2333 - acc: 0.9715 - val_loss: 0.4317 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00342: val_loss improved from 0.43178 to 0.43167, saving model to ./params.hdf5\n",
      "Epoch 343/500\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.2332 - acc: 0.9720 - val_loss: 0.4312 - val_acc: 0.8411\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.43167 to 0.43121, saving model to ./params.hdf5\n",
      "Epoch 344/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2327 - acc: 0.9724 - val_loss: 0.4311 - val_acc: 0.8443\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.43121 to 0.43108, saving model to ./params.hdf5\n",
      "Epoch 345/500\n",
      "19852/19852 [==============================] - 13s 651us/step - loss: 0.2316 - acc: 0.9725 - val_loss: 0.4311 - val_acc: 0.8411\n",
      "\n",
      "Epoch 00345: val_loss improved from 0.43108 to 0.43106, saving model to ./params.hdf5\n",
      "Epoch 346/500\n",
      "19852/19852 [==============================] - 13s 651us/step - loss: 0.2315 - acc: 0.9727 - val_loss: 0.4307 - val_acc: 0.8447\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.43106 to 0.43066, saving model to ./params.hdf5\n",
      "Epoch 347/500\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.2305 - acc: 0.9736 - val_loss: 0.4304 - val_acc: 0.8433\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.43066 to 0.43044, saving model to ./params.hdf5\n",
      "Epoch 348/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2300 - acc: 0.9727 - val_loss: 0.4297 - val_acc: 0.8445\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.43044 to 0.42969, saving model to ./params.hdf5\n",
      "Epoch 349/500\n",
      "19852/19852 [==============================] - 13s 653us/step - loss: 0.2302 - acc: 0.9716 - val_loss: 0.4296 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.42969 to 0.42958, saving model to ./params.hdf5\n",
      "Epoch 350/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2288 - acc: 0.9722 - val_loss: 0.4294 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.42958 to 0.42938, saving model to ./params.hdf5\n",
      "Epoch 351/500\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.2275 - acc: 0.9746 - val_loss: 0.4290 - val_acc: 0.8449\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.42938 to 0.42901, saving model to ./params.hdf5\n",
      "Epoch 352/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2278 - acc: 0.9742 - val_loss: 0.4289 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.42901 to 0.42892, saving model to ./params.hdf5\n",
      "Epoch 353/500\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.2272 - acc: 0.9740 - val_loss: 0.4283 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00353: val_loss improved from 0.42892 to 0.42832, saving model to ./params.hdf5\n",
      "Epoch 354/500\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.2263 - acc: 0.9722 - val_loss: 0.4281 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.42832 to 0.42809, saving model to ./params.hdf5\n",
      "Epoch 355/500\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.2266 - acc: 0.9733 - val_loss: 0.4281 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00355: val_loss did not improve\n",
      "Epoch 356/500\n",
      "19852/19852 [==============================] - 13s 650us/step - loss: 0.2239 - acc: 0.9747 - val_loss: 0.4278 - val_acc: 0.8427\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.42809 to 0.42776, saving model to ./params.hdf5\n",
      "Epoch 357/500\n",
      "19852/19852 [==============================] - 13s 652us/step - loss: 0.2249 - acc: 0.9715 - val_loss: 0.4273 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.42776 to 0.42734, saving model to ./params.hdf5\n",
      "Epoch 358/500\n",
      "16896/19852 [========================>.....] - ETA: 1s - loss: 0.2230 - acc: 0.9732"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ecd7c8934b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model_avg.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=500 #epochs\n\u001b[0;32m----> 2\u001b[0;31m                         ,initial_epoch=300, validation_split=0.2, shuffle=True, callbacks=[save_model])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model_avg.fit(x_train, y_train, batch_size = batch_size, verbose=1, epochs=500 #epochs\n",
    "                        ,initial_epoch=300, validation_split=0.2, shuffle=True, callbacks=[save_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot history\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    # 精度の履歴をプロット\n",
    "    plt.plot(history.history['acc'],label=\"accuracy\")\n",
    "    plt.plot(history.history['val_acc'],\"o-\",label=\"val_acc\")\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # 損失の履歴をプロット\n",
    "    plt.plot(history.history['loss'],label=\"loss\",)\n",
    "    plt.plot(history.history['val_loss'],\"o-\",label=\"val_loss\")\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8VPXV+PHPmZnsQBLCvoMCsiOL\nYrVuqMUNtS6g1ketSq27tu51rf3VLtpqbbW2j/uCiKLUB1FB1KqghEUQkFWWsIYkhISsM3N+f9yb\ncRIyyRAyZJnzfr0gc/dzZ5J75rvc7xVVxRhjjAHwNHUAxhhjmg9LCsYYY0IsKRhjjAmxpGCMMSbE\nkoIxxpgQSwrGGGNCLCmYuCIiL4jII1Guu1FETol1TMY0J5YUjDHGhFhSMKYFEhFfU8dgWidLCqbZ\ncattbheRZSKyT0T+V0Q6i8j7IlIkInNEJDNs/YkiskJE9ojIJyIyKGzZkSKy2N3uDSC5xrHOEpGl\n7rZfisjwKGM8U0SWiMheEdkiIg/WWH6cu7897vIr3PkpIvKYiGwSkUIR+dydd6KI5NTyPpzivn5Q\nRKaLyCsishe4QkSOEpH57jG2i8hTIpIYtv0QEflIRPJFZKeI3CMiXUSkRESywtYbLSK5IpIQzbmb\n1s2SgmmuzgdOBQYAZwPvA/cAHXB+b28CEJEBwOvALUBHYBbwHxFJdC+Q7wAvA+2BN9394m47CngO\n+AWQBfwTmCkiSVHEtw/4HyADOBP4pYic6+63lxvv39yYRgJL3e3+DIwGfuTGdAcQjPI9OQeY7h7z\nVSAA3Oq+J8cA44Hr3BjaAnOA2UA34HBgrqruAD4BLgrb78+AqapaGWUcphWzpGCaq7+p6k5V3Qr8\nF/hKVZeoajkwAzjSXW8S8H+q+pF7UfszkIJz0R0HJAB/VdVKVZ0OLAw7xjXAP1X1K1UNqOqLQLm7\nXZ1U9RNVXa6qQVVdhpOYTnAXXwrMUdXX3ePmqepSEfEAPwduVtWt7jG/dM8pGvNV9R33mKWqukhV\nF6iqX1U34iS1qhjOAnao6mOqWqaqRar6lbvsRZxEgIh4gYtxEqcxlhRMs7Uz7HVpLdNt3NfdgE1V\nC1Q1CGwBurvLtmr1UR83hb3uDfzKrX7ZIyJ7gJ7udnUSkaNFZJ5b7VIIXIvzjR13H+tr2awDTvVV\nbcuisaVGDANE5D0R2eFWKf2/KGIAeBcYLCL9cEpjhar6dQNjMq2MJQXT0m3DubgDICKCc0HcCmwH\nurvzqvQKe70F+J2qZoT9S1XV16M47mvATKCnqqYDzwBVx9kCHFbLNruBsgjL9gGpYefhxal6Cldz\nSOOnge+A/qraDqd6rb4YUNUyYBpOieYyrJRgwlhSMC3dNOBMERnvNpT+CqcK6EtgPuAHbhIRn4j8\nFDgqbNt/Ade63/pFRNLcBuS2URy3LZCvqmUichRwSdiyV4FTROQi97hZIjLSLcU8BzwuIt1ExCsi\nx7htGGuAZPf4CcBvgPraNtoCe4FiETkC+GXYsveALiJyi4gkiUhbETk6bPlLwBXAROCVKM7XxAlL\nCqZFU9XVOPXjf8P5Jn42cLaqVqhqBfBTnItfAU77w9th22bjtCs85S5f564bjeuAh0WkCLgfJzlV\n7XczcAZOgsrHaWQe4S7+NbAcp20jH/gD4FHVQnef/8Yp5ewDqvVGqsWvcZJREU6CeyMshiKcqqGz\ngR3AWuCksOVf4DRwL3bbI4wBQOwhO8bEJxH5GHhNVf/d1LGY5sOSgjFxSETGAh/htIkUNXU8pvmw\n6iNj4oyIvIhzD8MtlhBMTVZSMMYYE2IlBWOMMSEtblCtDh06aJ8+fZo6DGOMaVEWLVq0W1Vr3vuy\nnxaXFPr06UN2dnZTh2GMMS2KiGyqfy2rPjLGGBPGkoIxxpgQSwrGGGNCLCkYY4wJsaRgjDEmxJKC\nMcaYEEsKxhhjQlrcfQrGGBMPVJUPVuxkfW4x/TqkcfKgTiT5vDE/riUFY4xpBKpK+EP+cgpKeGvR\nVoZ0a8f4QZ3YXVzBO0u2curgzuwtq6RbRgprdhYxtHs6a3YUkb+vgt/+30r+fskotu0ppV1KAte+\nsii0v/6d2vCnC0cwsmdGTM/DkoIxxhygzXklbCssZVy/LAD+9MF3vLt0G3+ZNJLp2TncfcYR/PKV\nxSzfWgjAlOP7sXTzHr7emM/vZq2qti8RCB+XdMpLi9ixt4zM1AQSfR6+vOtkFm8q4IGZK9hRWAqW\nFIwxpvnI31fBpGfns7u4nI9/dSKfrN7FPz5Zjypc/+pidhWVM2v5dorK/fz9klF8tiaXZz/bAMBd\npx+BR6Bzu2S25JfQOyuNpVv2MKRbO1bvLGJrQSnvLdsOQEFJJT8Z0pkObZI4bUgXfty/IymJVn1k\njDExo6o8Ovs7SsoDnDakM2N6t4944Z35zTa+/j6PpVv2kFdcQWVAmfDXz9hXEWBIt3as3VXMrqJy\nhnVPxyNw9Y/7cebwrpwxrAuX/6gPAIO7tdtvv2eP6BZ6vXZnEZ+uzuWXJx3GYx+u4aejeoSWHYqE\nAC3weQpjxoxRGxDPGHOwSisC/GH2d7zw5UZ8HsEfVIb3SGfaL47hN+98S2llgK7tkgmo8rNxvTnj\nif9S7g/SJsnHkxeP5KX5m/h0TS6PXzSCc0d258oXFvLJ6lxmXPcjjuyV2eC4qtom9pRUkJGa2Gjn\nKyKLVHVMfetZScEY0+JtyS+hTZKP26d/w5Bu6fz82L7sLCqjd1ZqtR47qsqs5TtYub2QN7Nz2FVU\nzv8c05tfnTaQmd9s4753vuXkP3/CtsIyPAJB9zvz619vJtHnYdbNP6ZLu2TSknyM6JHBxrwSRvd2\nEsDVx/UjKy2JET0Ors6/qrG6MRPCAR3fSgrGmKaiqny4cifLcvZwyykDSPD+cOtUhT/Ixrx9DOjc\nlgp/kLmrdnLiwE4s2VLAqF6ZJCc4F/v/fLONG19fQttkH/vK/QQVEr0eKgJBTh3cGYBzR3ZnU/4+\nZn+7g2U5TuPv2D6Z3DHhCMb2aR865u9nrWLJ5j1MHNmNQV3bUlYZZP76PFbvLOKWU/ozpFv6IXx3\nGpeVFIwxzd5L8zfxwMwVAHRpl4zX42HC0C48MWcNM7/ZRkFJJY9fNIK1u4p5+pP19OuYxobcfYzo\nkc4Zw7qyemcRM5duY0DnNmzJL+WOCUdwfP+OvPb1JvL3VTBr+Q4APl2dS0UgyJBu7fjtOUO4+Khe\n+Lz737t79xmD9pt37OEdYvsmNDNWUjDGNKot+SWUVAQY2KVtxHXKKgMs2byHm6cuoW+HNLYVlrIl\nvxSATm2TyNtXwRnDurJx9z7W7SqmzB+gc9tkduwtY2yfTNbuKmZPSSVtknycd2R3fn3aQFKTvNVK\nGv5AkD/M/o7uGSk88n+rGNEzg2m/OAavRyKF1ao1i5KCiEwAngC8wL9V9dEay3sDzwEdgXzgZ6qa\nE8uYjDGNT1XZubecxz5czdtLthIIKkf3bU9ako+u6cnsLi5nXL8svt26ly35JXTLSOadpdsA+Ouk\nkSzfWsjv3/+O/p3asHZXMb8+bQA3nNyf9bnF3P32ckb2zOD6Ew/nkzW7OGVQZ1ITvRSV+2mb5Kt2\nw1g4n9fDvWcOBuCovll0z0yJ24RwIGJWUhARL7AGOBXIARYCF6vqyrB13gTeU9UXReRk4EpVvayu\n/VpJwZjGNX99Hod1SqNT22QCQcUjVLvQvrxgE7O/3c6Azm2Zcnw/3l++A69H6JaRgqry5fo8Vu8o\nYm9ZJet2FXPp0b0RcfZbVF7Jrr3lpKcksKuonNREL5WBIJUB5ewR3fjJkM6cOawrqrBy+176dEhj\n3ne7OH1ol1qrd0zDNYeSwlHAOlXd4AY0FTgHWBm2zmDgVvf1POCdGMZjjAnz7dZCNuWVcMPrizlz\nWFceOHsIFzzzJacP7cpdpx8BwMff7eS+d77lsI5pLNiQz/NfbKy2D59HCKiG7sj928VHVut3r6r4\ng4pHhLzictJTE3hnyVZe+2ozj5wzlPTUBMC5q3dod6cRN3x7c+jFMil0B7aETecAR9dY5xvgfJwq\npvOAtiKSpap54SuJyBRgCkCvXr1iFrAxrVX2xnyyNxXwfe4+Du/Uhp7tU7ju1cWhLpcfrtjJ1j2l\nbMor4YUvv2fFtkI6tkni49W7GNS1He9c/yO2FpTy4cqd9MhMYVNeCXvLKpmenUO5P8jPxvWmYF8F\nZw3vWu24IkKC1yl1dGqXDMCksb2YNNb+jpurWFYfXQj8RFWvdqcvA45S1RvD1ukGPAX0BT7DSRBD\nVLUw0n6t+sjEu+JyP1O/3syksT2Zvz6Pk47oRCCoPPSfFcz+dgcDOrdlyeY9/Lh/B249dQArt+3l\njreWAZCZmkBBSSUAvdqnMmlsT7pnpHDLG0vxCNxwcn+enLs2NB5PRmoC715/LL2z0mqNZc3OIorL\n/Yw6iJu1zKHRHKqPcoCeYdM9gG3hK6jqNuCnACLSBji/roRgTDzYW1bJks17OGFAR8oqA/x37W6O\nOSyLNknOn+vL8zfxh9nf8cqCTWzMK+HeMwZRGQzy+tdbOG1wZ1bt2MupQzrz3zW5nPW3zwE49vAs\n/nHJaNJTE9i4ex/TF+Vw5vCuDOrqDLuwYfc+Bndty4ShXfEI9O2QRkZqIl3aJUdMCAADOkfuYWRa\npliWFHw4Dc3jga04Dc2XqOqKsHU6APmqGhSR3wEBVb2/rv1aScG0RuHDLt/3zre8vGATb157DPPX\n5/H4R2ton5bI/ztvGP/8bD07C8vYVlgGQIJXSEnwIiIc2SuDF648KrTPwpJKZizJIdHnZeLIbqGk\nYuJTk5cUVNUvIjcAH+B0SX1OVVeIyMNAtqrOBE4Efi8iilN9dH2s4jGmOarwB7lnxnLeW7aN4T0y\nOPawDry12OmV/acPVrNzbxlHdGnLul3FXP/aYgJuI8CvTh1AYWklpw7uzN1vL6ekIsDtPxlYbd/p\nqQlccWzfQ35OpmWzm9eMaURb95SyflcxHdokcVinNPwBZXthKf6g8o9569mxt4xbTxnArOXb2VVU\nRl5xBdmbCjhzeFe25JeEhmC49OhevPrVZgD+dMFwvvo+n+mLcrj6uL4kJXi44aT+h2zUTNM6NHlJ\nwZjWasW2QhK8nlB9+uodRXy/ex+jemVw2f9+xYbcfQCM6Z1JapKPz9bkAk5Vj0eEi/+1gCSfh7bJ\nCZRU+Hli8kjOGdkdcIZO3pRXwvhBnTiiS1tmr9jB6cO6clz/DnRsm8TN4/uHxvwxJhaspGBMmMpA\nkMc/WsPpQ7vwpw9Wk72xgGt+3JfbThvI4x+upn/nttzz9nKKyv1MGtOTlEQvL87fiCqkJnopqQhw\nx4SBlJQHeGreOgAmj+3JMYdlMapXJgUlFSzLKeSs4V1pk+RDodrQDMbEipUUjIlSIKg8/8X3/GRI\nF6Yu3MzTn6znlfmbKCr3M7JnBk9+vI7c4gpe/3pzqKvm2SO68Ub2FkTg8mP6cMLAjlz/6mJ6tk9h\nyo/7ocCMJVvZW1rJPWcOol2yc5NWz/apDD/IoZWNiSVLCqbVW7uziMM7tUFEUFU+X7ebssogR/Vp\nz6LN+azctpc/f7iGV7/azMa8ffTrkMaG3c5NXm9eewxTXsrm9a830zbZR1GZnwGd2/Dk5JFcenQv\n2iT5QnfizrzhWLweT2h4hn9cOop9Ff5QQjCmJbCkYFqVqoeoTF24mV+fNpCcglKuf20xvzlzEFcd\n15e/zFnLk3PXAlR7iEpWWiLf795Hv45pzLjuWG5/8xsuHNOTBK+Hp382mt++t5KTj+jExrwSBnVp\ni4iEHtpe5fBO1fvsj4jxA9aNiQVrUzAtWjCofLY2l7LKAEO7p3Pz1KUs2lSAzyN4PEJWWiLbC8tI\n8nlol5JAblE554/qwfmjuvPBih0c2SuTVTv2MnlsL+au2sn4QZ3p2yHyzVrGtFTRtilYUjDN0rKc\nPSzeVLBfP/vC0kq+2pDHqN6ZzF21k9U7innui+8BZwiHyoBy1+lH8JMhXbjx9cUs2JDPbacOIHtT\nAVlpiRx3eAcmjuxmjbsm7lhDs2mRAkGluNzP4x+t4ZPVuZw2pAvdMlJCy/8w+zte+2ozXdo5D1wB\nuHB0D4IKby3O4U8XDOfCMc7oKi9fdTSLNhVwdN/2EcfcN8ZUZ0nBNBuqytUvLmT51r3sLXUGbZux\nZCuDurZlWU4hxWV+Zi7dhs8j7NhbxlXH9WVfuZ/7zhpMos/D5T/qzbDuPzxDN8Hr2a/e3xhTN0sK\nJuYWbSogNdEbqqsPv/kqr7ic95ZtJyM1gaIyP/NW54aWJXo9/OmD1YAz3r5HxOk+euVYkn1ejjms\n+gXfunoac/AsKZiYKSyp5Mv1u7l56lK6pCfTpV0yQVXevPYYRITZ3+7g9unfUFTmx+sR0lMSOLpv\ne3pnpfLftbu58eT+vDR/I7f/ZCCje2eyYfc+sjfmc+KAjlYdZEyMWEOzOSirtu+le2YK7ZITyN6Y\nj8fjfJtPSfBy1YsL2bm3nOQED2WVwdA2N43vzwkDOnD1i9l0y0jh3jMGcdWL2ZRWBnj9mnGM7ZNJ\nmT9oo3oa04isodnE3OodRZz9t88Z3TuTZ342mkv//RXl/h8u/uf5vuCRzLdJLd3Bdm8W//BcSna7\nU9g473ku/+9LLJZiyAd5BVZ6YV9iOmn7/ox4L6JNQ3oHLZsGcx+GwhxI7wHj74fhFzXiGRvT+llJ\noTVryEVy2TR4/04ozXemU9rD6X+otl1ZZYBL/rWAgbtmc7s+RybFIIDi/AwTPqkR5sdUYhoMnwxr\nP6z+PkDk96a+960lJp+WGLNpVHafQjyqeUGPhnhAg/xwVY9Ma07IIby4t3S1JNeIDvQCXnP9/qe5\nSXALiBc0cOBxRfpdOpDziPZ8DlXCivVx6tp/M0jKlhTiRUMSgWk9PIkQrGjqKJpG1Rea9J7VE2EU\nX3AaTUIa+JLcv78YHDd8/1UJPr1ng5KKtSm0ZpYITJV4TQjglnBxEkH2/4YvOHQxVO5z/sXquOH7\nryrxFW6B/9zkvI5BacOSQnNmF39jTG0qS53qKEsKccKSgTGmPoU5MdmtJYXmIh4TQVWjJcTfuRtz\nsNJ7xGS3lhSawiFOABr6zxVNr6GENAhURldn3ZAeKTUdbDE4qh44h7AB0piYkh+6VjcySwqxFn6x\nSskEf3lYw1RsVHhSKA54yZB9bAtm8Uf/RZQPOp+5q3bhDyqvXHU0x/XvsH98LblP/vCLGieu0Plu\nqd7bo7YkE00vkKYsAVb1zqkt3mjjSkyDs/7qvLbSXDMhMObnMfs7tC6psbRsmtNLoLL0kBxOfSnI\nxCeZ+Fk3yiuDvHrN0Yx5ZA4AL/38KEb2yiB7Yz4nDexkYwc1BweTcGORrBt6s+Oh/tIQnrhrlv7C\nu6nGMpa6kmqoG2lB9ffkvdsg+7nq8SakwNlP1n+fSCOcl92n0FSq/cLGRtUnJglplJNAQkUh2zSL\n1UNvIa/vudzx1jIemjiEy3/Uh3W7inh5/ibuOXMQST5vnfs1xsRYE5a8LSk0hdq+CTQa5xtRsF0P\n/lg5ief2jmXBPeM5+2+f06FNIuX+IN/tKAKge0YKs27+Mekp9sB4Y4zDbl471N67rcYNNI2gliLj\n/e8s55UFm4EgN7y2mK17SnnkvKFs31PG72et4nc/HcaEIV1I9NnjJo0xB86SQmNozIRQR0+ej1bu\n5JUFm7nquL68t2wbX67PY1y/9qHnC0wa2xOvx9oKjDENZ0nhYDVGQkjvCbd+W+siVSW3qJwtBaXc\nMf0bhnRrxx0TBiLA619v5o/njwg1GltCMMYcLEsKB2PZNLcN4SAkpFTrbxwIOu0RXo+weHMBD85c\nwbKcQgCSEzw8MflIknxebp8wkCkn9KNT2+SDO74xxoSxpHAw3r+Tg2pUDqsqUlVEhCkvZZO3r4Jx\n/bJ45tP1dG6XxG/OHES3jBSO6NKWfh3bAJDk89KprfUmMsY0LksKDfXebdHdyFPVSAy1dkV7Z8lW\n0lbu5J0lW1m9s4h1u4oBWLplDxeM7sFDE4eQZo+lNMYcIna1aYhoqo3GXAVnPV59Xo3G411FZdzx\n1jKSvB6Kyv0AdGmXzCmDO1Gwr5JHfzoMX0MeS2mMMQ1kSaEh6qs2qi0hhKkMBPGK8NznG6nwB6nw\nB/F6hFevPpou7ZLp0yGt8WM2xpgoWFI4UMum1V1tlNK+zoQwa/l27nvnW7qkJ7NmZxHnjuzGvooA\n6SkJjOuXFYOAjTEmepYUDtTch+tYKD8MBR3GHwhy/jPzGdqtHdMX5dAnK421u4rplpHCQ+cMtTuP\njTHNhiWFA1XXmEYRRi58e/FWvtmyh2+27AHgb5ccSZLPQ1qSzxKCMaZZsaRwIJZNI+KY/BGqjZZu\n2cMfP1jNwM5t2VJQwujemQzo3DbmoRpjTEPENCmIyATgCcAL/FtVH62xvBfwIpDhrnOXqs6KZUwH\nZe7D1N7AXHu10dY9pVz6rwVkpiXyxMUjEYT2aYkxD9MYYxoqZklBRLzA34FTgRxgoYjMVNWVYav9\nBpimqk+LyGBgFtAnVjEdtIhVR1qt2qjCH+SqFxeydmcxQYXXrxlHz/aphyZGY4w5CLHsBH8UsE5V\nN6hqBTAVOKfGOgq0c1+nA9tiGM/BCVUd1SK9Z7XJ177axH/X7iYjNYGHJg6xhGCMaTFiWX3UHQj/\nap0DHF1jnQeBD0XkRiANOKW2HYnIFGAKQK9evRo90KjUVXXk3rFcUuGnoKSSJ+au5Zh+Wbx2zdH2\nhDNjTIsSy6RQ29Ww5lX1YuAFVX1MRI4BXhaRoaoarLaR6rPAs+A8ZCcm0dannqqj5TmFTH52PuX+\nIMkJXn577hBLCMaYFieWSSEHCK9X6cH+1UNXARMAVHW+iCQDHYBdMYzrwNXV6yi9Jws25HHDa0vI\nSE3kyF4ZTBrbk8M7WQ8jY0zLE8uksBDoLyJ9ga3AZOCSGutsBsYDL4jIICAZyI1hTA1TR9VRyfH3\ncvlzX9MtI4V//c9oSwbGmBYtZg3NquoHbgA+AFbh9DJaISIPi8hEd7VfAdeIyDfA68AV2hwfGl2Y\nE2GBsqbT6ZT7g9xzxiBLCMaYFi+m9ym49xzMqjHv/rDXK4FjYxlDo0jvUXubQnpPvt/tDHXd1wax\nM8a0AjYuczT6n7bfLE1I4cOuv2DB+nw8Ar2s26kxphWwYS7qs2wafPNajZlCQf8LmLK4H7CFXu1T\nSfRZfjXGtHx2JavP+3dCZWmNmUry93NCU1Z1ZIxpLSwp1KWOZyeklO4IvbakYIxpLSwp1KWOZyfk\neTvSsW0SiV4PQ7q1i7ieMca0JNamUJeIXVHhKbmEcf2y+M2Zg+jQJukQBmWMMbFjJYW6pPeodXYw\nOZMXio/iiC5t6dwuGa/HhrMwxrQOlhTqMv5+8NZ4MlpCCgsH3QXAUX3bN0FQxhgTO5YU6jL8IhhU\ndfO1OENkn/0kf905kt5ZqYzpndmk4RljTGOzNoX6pPcEjw/u2w0i7CgsY/5rc7nt1AE2CqoxptWx\nkkJ9SvOd5y+7CeDL9bsBOGVQ56aMyhhjYsKSQn1K8iH1h7aDBRvySE9J4IguNvidMab1saRQn9IC\nSM0KTS7YkM/RfdvjsR5HxphWyJJCfUryIcVpUN6Ut4/N+SUc3S+rno2MMaZlsqRQn9Ifqo+e/2Ij\nPo9wxrAuTRyUMcbERlRJQUTeEpEzRSS+koiqW1JoT2FpJW8s3MI5I7vTNT2lqSMzxpiYiPYi/zTO\nozTXisijInJEDGNqPpa8DMFK+OKvJP5tOKcGPuXCMbXf5WyMMa1BVElBVeeo6qXAKGAj8JGIfCki\nV4pIQt1bt1DLpsGs20OTKSXbeDTh3xy556MmDMoYY2Ir6uogEckCrgCuBpYAT+AkidZ5lZz7MPjL\nqs1KlQqSPn2kiQIyxpjYi+qOZhF5GzgCeBk4W1W3u4veEJHsWAXXpCKNkFrHyKnGGNPSRTvMxVOq\n+nFtC1R1TCPG03yk94DCLbXPN8aYVira6qNBIpJRNSEimSJyXYxiah7G3w/e6s9JUF+KM98YY1qp\naJPCNaq6p2pCVQuAa2ITUjMx/CI4+hcABBWKk7siE5905htjTCsVbVLwSNiQoCLiBRJjE1Iz0usY\nACZWPMI3F3xuCcEY0+pF26bwATBNRJ4BFLgWmB2zqJqLoB8APz6SE+Lrvj1jTHyKNincCfwC+CUg\nwIfAv2MVVLMRSgoeknzeJg7GGGNiL6qkoKpBnLuan45tOM2MmxQCeK2kYIyJC9Hep9Af+D0wGEiu\nmq+q/WIUV/PgJoVKvFZSMMbEhWi//j6PU0rwAycBL+HcyNa6BSqdH+olyUoKxpg4EO2VLkVV5wKi\nqptU9UHg5NiF1UyEtSkkJ1hJwRjT+kXb0FzmDpu9VkRuALYCnWIXVjMRDADgx0uSz0oKxpjWL9or\n3S1AKnATMBr4GXB5rIJqNoJu9ZF4SfRaUjDGtH71lhTcG9UuUtXbgWLgyphH1Vy41Uc+XwJh9+4Z\nY0yrVe/XX1UNAKMlHq+KblLw+lr/zdvGGAPRtyksAd4VkTeBfVUzVfXtmETVXAScpJDga53PETLG\nmJqiTQrtgTyq9zhSoHUnBbekkJAQ7dtkjDEtW7R3NMdPO0K4oN8d98iSgjEmPkR7R/PzOCWDalT1\n5/VsNwHnsZ1e4N+q+miN5X/BuRkOnN5NnVQ1g+YiWElA7MY1Y0z8iPYr8Hthr5OB84BtdW3g9lr6\nO3AqkAMsFJGZqrqyah1VvTVs/RuBI6OM59AIBpxxj2yIC2NMnIi2+uit8GkReR2YU89mRwHrVHWD\nu81U4BxgZYT1LwYeiCaeQybod25cs5KCMSZONPRq1x/oVc863YHwhxznuPP2IyK9gb5Arc+BFpEp\nIpItItm5ubkNCLeBApUEbDA8Y0wcibZNoYjqbQo7cJ6xUOdmtczbr13CNRmY7t4Tsf9Gqs8CzwKM\nGTMm0j4aX9BPAI8Nm22MiRvRVh+1bcC+c4CeYdM9iNwOMRm4vgHHiK1ggEq1koIxJn5E9RVYRM4T\nkfSw6QwRObeezRYC/UWkr4jwee+CAAAVWElEQVQk4lz4Z9ay74FAJjA/+rAPkWAlfnvAjjEmjkR7\ntXtAVQurJlR1D/U0CquqH7gB5/nOq4BpqrpCRB4WkYlhq14MTFXVQ1ctFK2gn0q1R3EaY+JHtF1S\na0se9W6rqrOAWTXm3V9j+sEoYzj0gn4q1UoKxpj4Ee3VLltEHheRw0Skn3vT2aJYBtYcBP2V9oAd\nY0xciTYp3AhUAG8A04BSmmPDcCMLBvz2gB1jTFyJtvfRPuCuGMfS7AQDVQ3NVlIwxsSHaHsffSQi\nGWHTmSLyQezCah7UTQpWUjDGxItor3Yd3B5HAKhqAXHwjOZgwE9ArU3BGBM/ok0KQREJDWshIn2I\nfHdyq6Fum4L1PjLGxItou6TeC3wuIp+608cDU2ITUjMSqj6ykoIxJj5E29A8W0TG4CSCpcC7OD2Q\nWjUN+vGTSJq1KRhj4kS0A+JdDdyMM37RUmAczrAUJ9e1XYvnDp3t89Q2tp8xxrQ+0X4FvhkYC2xS\n1ZNwHoZzCMewbhrijpKaYCUFY0yciPZqV6aqZQAikqSq3wEDYxdWMxH0U4mPBI8lBWNMfIi2oTnH\nvU/hHeAjESmgnsdxtgaiAQLqIcFn1UfGmPgQbUPzee7LB0VkHpAOzI5ZVM2EuENn+6ykYIyJE9GW\nFEJU9dP612olggH8eEn0WlIwxsQHu9rVQdTtfeS16iNjTHywpFAHT1XvIyspGGPihF3t6iAawI+P\nBCspGGPihCWFOjhJwUoKxpj4YVe7SFTxqp+AtSkYY+KIJYVIggEAKtVrN68ZY+KGXe0iCfqdH+LF\nY2MfGWPihCWFSIKVzk+PDZttjIkflhQiCZUUEpo4EGOMOXQsKUQScJKClRSMMfHEkkIkbklB5YBH\nAjHGmBbLkkIkVUnBY0nBGBM/LClEEmpotqRgjIkflhQice9TwGtJwRgTPywpRBKqPrLeR8aY+GFJ\nIZKAU33ksd5Hxpg4YkkhkjXOg+XuKvod/GUoLJvWxAEZY0zsWVKozbJp8NmfARCAwi3wn5ssMRhj\nWj1LCrWZ+zAEyqvPqyx15htjTCtmSaE2hTkHNt8YY1oJSwq1Se9xYPONMaaVsKRQm/H3gzex+ryE\nFGe+Mca0YpYUajP8Ihh7NQAKkN4Tzn7SmW+MMa1YTJOCiEwQkdUisk5E7oqwzkUislJEVojIa7GM\n54D0PAqAxw57AW791hKCMSYuxGwMBxHxAn8HTgVygIUiMlNVV4at0x+4GzhWVQtEpFOs4jlg7tDZ\nHq/d0WyMiR+xLCkcBaxT1Q2qWgFMBc6psc41wN9VtQBAVXfFMJ4D4w5z4fHZ2EfGmPgRy6TQHdgS\nNp3jzgs3ABggIl+IyAIRmVDbjkRkiohki0h2bm5ujMKtwR0lVWo2OBtjTCsWy6RQ29Putca0D+gP\nnAhcDPxbRDL220j1WVUdo6pjOnbs2OiB1qpq7COfJQVjTPyIZVLIAXqGTfcAttWyzruqWqmq3wOr\ncZJEk1M3KXh91qZgjIkfsUwKC4H+ItJXRBKBycDMGuu8A5wEICIdcKqTNsQwpqgFraRgjIlDMUsK\nquoHbgA+AFYB01R1hYg8LCIT3dU+APJEZCUwD7hdVfNiFdOBCAasodkYE39iesVT1VnArBrz7g97\nrcBt7r9mJeCvIAHwWUnBGBNH7I7mCNTvVh/ZfQrGmDhiSSGCgL8Cv3pISLAnrxlj4oclhQg0UIkf\nLwkee4uMMfHDrngRBP1uUvDVdruFMca0TpYUIqgqKfispGCMiSPW3zKCYKCSAF4SvJYUjDHxw654\nETglBR8JXqs+MsbED0sKEWigEr9aScEYE1/siheB+ivx47GkYIyJK3bFiyAY8OPHR2qi3adgjIkf\nlhQiqOp9ZEnBGBNPLClEEPRXUImXFEsKxpg4YkkhkqBTUkixYS6MMXHE7lOIQAN+t/rI3iJjDqXK\nykpycnIoKytr6lBapOTkZHr06EFCQsMG87QrXiRul9TkBCtMGXMo5eTk0LZtW/r06YOI3Sd0IFSV\nvLw8cnJy6Nu3b4P2YVe8SIJ+gh6f/VIac4iVlZWRlZVlf3sNICJkZWUdVCnLkkIkwUpUrCBlTFOw\nhNBwB/veWVKIQIJ+1GMP2DHGxBdLChFI0A8e63lkjIkvlhQi8KgftUdxGmNixO/3N3UItbJK8wgk\n6IcGdukyxjSOh/6zgpXb9jbqPgd3a8cDZw+pc51zzz2XLVu2UFZWxs0338yUKVOYPXs299xzD4FA\ngA4dOjB37lyKi4u58cYbyc7ORkR44IEHOP/882nTpg3FxcUATJ8+nffee48XXniBK664gvbt27Nk\nyRJGjRrFpEmTuOWWWygtLSUlJYXnn3+egQMHEggEuPPOO/nggw8QEa655hoGDx7MU089xYwZMwD4\n6KOPePrpp3n77bcb9f2xpBCBVwOI194eY+LRc889R/v27SktLWXs2LGcc845XHPNNXz22Wf07duX\n/Px8AH7729+Snp7O8uXLASgoKKh332vWrGHOnDl4vV727t3LZ599hs/nY86cOdxzzz289dZbPPvs\ns3z//fcsWbIEn89Hfn4+mZmZXH/99eTm5tKxY0eef/55rrzyykY/d7vqReDBj3gTmzoMY+Jafd/o\nY+XJJ58MfSPfsmULzz77LMcff3yo73/79u0BmDNnDlOnTg1tl5mZWe++L7zwQrxep72ysLCQyy+/\nnLVr1yIiVFZWhvZ77bXX4vP5qh3vsssu45VXXuHKK69k/vz5vPTSS410xj+wpBCBV/1gbQrGxJ1P\nPvmEOXPmMH/+fFJTUznxxBMZMWIEq1ev3m9dVa21C2j4vJr3DKSlpYVe33fffZx00knMmDGDjRs3\ncuKJJ9a53yuvvJKzzz6b5ORkLrzwwlDSaEzW0ByBDz8eqz4yJu4UFhaSmZlJamoq3333HQsWLKC8\nvJxPP/2U77//HiBUfXTaaafx1FNPhbatqj7q3Lkzq1atIhgMhkockY7VvXt3AF544YXQ/NNOO41n\nnnkm1Bhddbxu3brRrVs3HnnkEa644opGO+dwlhQi8GkAj89KCsbEmwkTJuD3+xk+fDj33Xcf48aN\no2PHjjz77LP89Kc/ZcSIEUyaNAmA3/zmNxQUFDB06FBGjBjBvHnzAHj00Uc566yzOPnkk+natWvE\nY91xxx3cfffdHHvssQQCgdD8q6++ml69ejF8+HBGjBjBa6+9Flp26aWX0rNnTwYPHhyT8xdVjcmO\nY2XMmDGanZ0d02MEggoPZZLd60qOvuovMT2WMaa6VatWMWjQoKYOo9m64YYbOPLII7nqqqsirlPb\neygii1R1TH37t/qRWpRVVJImitfaFIwxzcjo0aNJS0vjsccei9kxLCnUoqS0jDTAm2C9j4wxzcei\nRYtifgxrU6hFWVk5AF5rUzDGxBlLCrUoq3C6kPl8VlIwxsQXSwq1qCop+Kz6yBgTZywp1KLEvdkk\nIdGSgjEmvlhSqMWGHXsA6JDepokjMcbUa9k0+MtQeDDD+blsWlNH1KJZUqjF6u3OXYltU5KbOBJj\nTJ2WTYP/3ASFWwB1fv7npkOaGNq0aV1fHq1Lai3W7XBuKbexj4xpYu/fBTuWR16esxAC5dXnVZbC\nuzfAohdr36bLMDj90caLsZWJaUlBRCaIyGoRWScid9Wy/AoRyRWRpe6/q2MZTzTyisvZXVjiTHgs\nZxrTrNVMCPXNj8Kdd97JP/7xj9D0gw8+yEMPPcT48eMZNWoUw4YN4913341qX8XFxRG3e+mll0LD\nWFx22WUA7Ny5k/POO48RI0YwYsQIvvzyywafR0PF7KonIl7g78CpQA6wUERmqurKGqu+oao3xCqO\naBXsq2Dud7v4cv1uvLhjkFhJwZimVd83+r8MdauOakjvCVf+X4MOOXnyZG655Rauu+46AKZNm8bs\n2bO59dZbadeuHbt372bcuHFMnDix1pFMwyUnJzNjxoz9tlu5ciW/+93v+OKLL+jQoUNowLubbrqJ\nE044gRkzZhAIBEIP6jmUYvlV+ChgnapuABCRqcA5QM2kcEgs2pTP52vz9ptfUuln/a5ivv4+n71l\nfiZ6PueNNm9AJTDzJqjYB8MvOvQBG2PqN/5+pw2hsvSHeQkpzvwGOvLII9m1axfbtm0jNzeXzMxM\nunbtyq233spnn32Gx+Nh69at7Ny5ky5dutS5L1Xlnnvu2W+7jz/+mAsuuIAOHToAPzwv4eOPPw49\nI8Hr9ZKent7g82ioWCaF7kB4Cs8Bjq5lvfNF5HhgDXCrqtaS9g/SsmkMfvfXjPIX1r68Ktm77cpS\n6U6X7HZ+4cASgzHNUdXf5dyHoTAH0ns4CeEg/14vuOACpk+fzo4dO5g8eTKvvvoqubm5LFq0iISE\nBPr06bPfcxJqE2m7SM9LaA5i2aZQ2xnXHJL1P0AfVR0OzAFqbRkSkSkiki0i2bm5uQcWxbJp8M51\npAQKEaH2f1T/V01lqfMLZ4xpnoZfBLd+Cw/ucX42whe4yZMnM3XqVKZPn84FF1xAYWEhnTp1IiEh\ngXnz5rFp06ao9hNpu/HjxzNt2jTy8pzai6rqo/Hjx/P0008DEAgE2Lu3cZ9PHY1YJoUcoGfYdA9g\nW/gKqpqnqlUtQv8CRte2I1V9VlXHqOqYjh07HlgUcx+GYGX969WlMOfgtjfGtChDhgyhqKiI7t27\n07VrVy699FKys7MZM2YMr776KkcccURU+4m03ZAhQ7j33ns54YQTGDFiBLfddhsATzzxBPPmzWPY\nsGGMHj2aFStWxOwcI4nZ8xRExIdTJTQe2AosBC5R1RVh63RV1e3u6/OAO1V1XF37PeDnKTyYwf4F\nlAOU3tP5BmKMiTl7nsLBa5bPU1BVv4jcAHwAeIHnVHWFiDwMZKvqTOAmEZkI+IF84IpGDyS9R+29\nE6ImB9VoZYwxLUlMO+Kr6ixgVo1594e9vhu4O5YxMP5+eOe6hlchjfm5NTIbY+q0fPny0L0GVZKS\nkvjqq6+aKKKGa/13Z1Vd0N+/E0rzo98upT2c/gdLCMY0gebcO6c2w4YNY+nSpU0dBuC8dwej9ScF\ncC7sdnE3pkVITk4mLy+PrKysFpUYmgNVJS8vj+Tkho/bFh9JwRjTYvTo0YOcnBwOuPu5AZyk2qNH\njwZvb0nBGNOsJCQk0Ldv36YOI27Z0NnGGGNCLCkYY4wJsaRgjDEmJGZ3NMeKiOQC0Q08sr8OwO5G\nDKcp2bk0T3YuzZOdC/RW1XrHCWpxSeFgiEh2NLd5twR2Ls2TnUvzZOcSPas+MsYYE2JJwRhjTEi8\nJYVnmzqARmTn0jzZuTRPdi5Riqs2BWOMMXWLt5KCMcaYOlhSMMYYExI3SUFEJojIahFZJyJ3NXU8\nB0pENorIchFZKiLZ7rz2IvKRiKx1f2Y2dZy1EZHnRGSXiHwbNq/W2MXxpPs5LRORUU0X+f4inMuD\nIrLV/WyWisgZYcvuds9ltYj8pGmi3p+I9BSReSKySkRWiMjN7vwW97nUcS4t8XNJFpGvReQb91we\ncuf3FZGv3M/lDRFJdOcnudPr3OV9DjoIVW31/3Ce/LYe6AckAt8Ag5s6rgM8h41Ahxrz/gjc5b6+\nC/hDU8cZIfbjgVHAt/XFDpwBvA8IMA74qqnjj+JcHgR+Xcu6g93ftSSgr/s76G3qc3Bj6wqMcl+3\nxXl07uCW+LnUcS4t8XMRoI37OgH4yn2/pwGT3fnPAL90X18HPOO+ngy8cbAxxEtJ4ShgnapuUNUK\nYCpwThPH1BjOAV50X78InNuEsUSkqp/hPG41XKTYzwFeUscCIENEuh6aSOsX4VwiOQeYqqrlqvo9\nsA7nd7HJqep2VV3svi4CVgHdaYGfSx3nEklz/lxUVYvdyQT3nwInA9Pd+TU/l6rPazowXg7yIRTx\nkhS6A+EPas6h7l+a5kiBD0VkkYhMced1VtXt4PxhAJ2aLLoDFyn2lvpZ3eBWqzwXVo3XIs7FrXI4\nEudbaYv+XGqcC7TAz0VEvCKyFNgFfIRTktmjqn53lfB4Q+fiLi8Esg7m+PGSFGrLnC2tL+6xqjoK\nOB24XkSOb+qAYqQlflZPA4cBI4HtwGPu/GZ/LiLSBngLuEVV99a1ai3zmvu5tMjPRVUDqjoS6IFT\nghlU22ruz0Y/l3hJCjlAz7DpHsC2JoqlQVR1m/tzFzAD55dlZ1UR3v25q+kiPGCRYm9xn5Wq7nT/\nkIPAv/ihKqJZn4uIJOBcRF9V1bfd2S3yc6ntXFrq51JFVfcAn+C0KWSISNVD0cLjDZ2Luzyd6Ks3\naxUvSWEh0N9twU/EaZCZ2cQxRU1E0kSkbdVr4DTgW5xzuNxd7XLg3aaJsEEixT4T+B+3t8s4oLCq\nOqO5qlG3fh7OZwPOuUx2e4j0BfoDXx/q+Grj1jv/L7BKVR8PW9TiPpdI59JCP5eOIpLhvk4BTsFp\nI5kHXOCuVvNzqfq8LgA+VrfVucGaurX9UP3D6T2xBqd+7t6mjucAY++H01viG2BFVfw4dYdzgbXu\nz/ZNHWuE+F/HKb5X4nyzuSpS7DjF4b+7n9NyYExTxx/FubzsxrrM/SPtGrb+ve65rAZOb+r4w+I6\nDqeaYRmw1P13Rkv8XOo4l5b4uQwHlrgxfwvc787vh5O41gFvAknu/GR3ep27vN/BxmDDXBhjjAmJ\nl+ojY4wxUbCkYIwxJsSSgjHGmBBLCsYYY0IsKRhjjAmxpGDMISQiJ4rIe00dhzGRWFIwxhgTYknB\nmFqIyM/cce2Xisg/3UHKikXkMRFZLCJzRaSju+5IEVngDrw2I+wZBIeLyBx3bPzFInKYu/s2IjJd\nRL4TkVcPdlRLYxqTJQVjahCRQcAknEEIRwIB4FIgDViszsCEnwIPuJu8BNypqsNx7qCtmv8q8HdV\nHQH8COdOaHBG8bwFZ1z/fsCxMT8pY6Lkq38VY+LOeGA0sND9Ep+CMzBcEHjDXecV4G0RSQcyVPVT\nd/6LwJvuWFXdVXUGgKqWAbj7+1pVc9zppUAf4PPYn5Yx9bOkYMz+BHhRVe+uNlPkvhrr1TVGTF1V\nQuVhrwPY36FpRqz6yJj9zQUuEJFOEHpucW+cv5eqkSovAT5X1UKgQER+7M6/DPhUnfH8c0TkXHcf\nSSKSekjPwpgGsG8oxtSgqitF5Dc4T7rz4IyIej2wDxgiIotwnnA1yd3kcuAZ96K/AbjSnX8Z8E8R\nedjdx4WH8DSMaRAbJdWYKIlIsaq2aeo4jIklqz4yxhgTYiUFY4wxIVZSMMYYE2JJwRhjTIglBWOM\nMSGWFIwxxoRYUjDGGBPy/wHUtfIQfmm+bgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6307833978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f63059cd240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_avg.save_weights('./params.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24808/24808 [==============================] - 9s 355us/step\n"
     ]
    }
   ],
   "source": [
    "score=model_avg.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.473302556435 0.838640761045\n"
     ]
    }
   ],
   "source": [
    "print(score[0],score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
