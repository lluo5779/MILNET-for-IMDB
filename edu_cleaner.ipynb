{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/owner/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import string\n",
    "from numpy import loadtxt\n",
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as multi\n",
    "sys.path.append(os.pardir)\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "# lines = loadtxt(\"/home/louis/SharedWindows/edu_data/\"+\"imdb-edus.train\")\n",
    "# train = []\n",
    "\n",
    "# with open(dir+\"imdb-edus.train\", 'r') as train_file:\n",
    "#     lines = train_file.readlines()\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "# '''text2index'''\n",
    "# \n",
    "# from func import TextDeal\n",
    "# from func import wrapper_func_four\n",
    "# \n",
    "# vocab = wList\n",
    "# \n",
    "# td = TextDeal()\n",
    "# load_path = sorted( glob.glob( '/home/dl-box/デスクトップ/PythonFile/hirose/Text/text_list/*.txt' ) )\n",
    "# name = [ os.path.splitext( os.path.basename( r ) )[0] for r in load_path ]\n",
    "# save_path = '/home/dl-box/デスクトップ/PythonFile/hirose/LSTM/W2I/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_func_six(tuple_data):\n",
    "    return(tuple_data[0](tuple_data[1],tuple_data[2],tuple_data[3],tuple_data[4], tuple_data[5], tuple_data[6]) )\n",
    "\n",
    "def readEduStyledFiles(dir, filename):\n",
    "    with open(dir+filename, 'r') as train_file:\n",
    "        lines = train_file.readlines()\n",
    "    return lines\n",
    "        \n",
    "def tokenizeListOfTexts(list, vocab):\n",
    "    idx = []\n",
    "    isEmpty = False\n",
    "    for word in list:\n",
    "        if word in vocab:\n",
    "            idx.append(vocab[word].index)\n",
    "            #idx.append(len(word))\n",
    "            #print(word)\n",
    "        # else:\n",
    "        #     print(word)\n",
    "    if idx == []:\n",
    "        isEmpty = True\n",
    "    return idx, isEmpty\n",
    "\n",
    "def addUniqueElementsToList(docs, li):\n",
    "    for doc in docs:\n",
    "        for sentence in doc:\n",
    "            for word in sentence:\n",
    "                if word not in li:\n",
    "                    #print(word)\n",
    "                    li.append(word)\n",
    "    return li\n",
    "\n",
    "def generateUniqueIndices(path_list):\n",
    "    vocab_list = []\n",
    "    for filepath in path_list:\n",
    "        tmp = np.load(filepath)\n",
    "        li = addUniqueElementsToList(tmp, vocab_list)\n",
    "        vocab_list = vocab_list + li\n",
    "    return vocab_list\n",
    "\n",
    "\n",
    "def cleanAndTokenizeEDUs(vocab, load_path=None, load_name=None, save_path=None, save_name=None, willReturn = True, max_sent_len = 15,max_doc_len = 6):\n",
    "    \n",
    "    if (willReturn == False) and (all(v is None for v in [load_path, load_name, save_path, save_name])):\n",
    "        raise ValueError('Function Arguments cannot all be None Type when saving files')\n",
    "    elif (willReturn == True) and (all(v is None for v in [load_path, load_name])):\n",
    "        raise ValueError('Load arguments cannot all be None Type when loading file')\n",
    "    \n",
    "    lines = readEduStyledFiles(load_path,load_name)\n",
    "\n",
    "    cleaned_lines = []\n",
    "    length = len(lines)\n",
    "    scores = []\n",
    "    ids = []\n",
    "    cleaned_doc=[]\n",
    "    cleaned_all=[]\n",
    "    idx_binary = [] # index for binary training only\n",
    "    scores_binary =[]\n",
    "    \n",
    "    doc_count = 0\n",
    "    sent_counter = -1\n",
    "    \n",
    "    for i in range(len(lines)):  \n",
    "        \n",
    "        if len(cleaned_all) in [114,116,135]:\n",
    "            a = 3\n",
    "               \n",
    "        sent_counter += 1\n",
    "        \n",
    "        cleaned = lines[i].replace('\\n','').replace('<s>','').replace(\"'\",'')\n",
    "        cleaned = text_to_word_sequence(cleaned)\n",
    "        \n",
    "#         if i in [91,92,94]:\n",
    "#             print(str(i)+' now enters')\n",
    "#             print(cleaned)\n",
    "#             print(sent_counter)\n",
    "        \n",
    "        if (len(cleaned_doc) <= max_doc_len):\n",
    "\n",
    "\n",
    "#                 #DEBUGCODE\n",
    "#                 if i in [91,92,94]:\n",
    "#                     print(str(i)+' exit at len = 0 for sent<max')\n",
    "                \n",
    "            if (len(cleaned) == 2): # and (i != 0):\n",
    "#                 #DEBUGCODE\n",
    "#                 if i in [91,92,94]:\n",
    "#                     print(str(i)+' exit at len = 2 for sent<max')\n",
    "                \n",
    "                if  (cleaned[0].isnumeric() and cleaned[1].isnumeric()):\n",
    "                    \n",
    "                    if i != 0:\n",
    "                        if(len(cleaned_doc) < max_doc_len):\n",
    "                            while (len(cleaned_doc) < max_doc_len):\n",
    "                                tokenized = [0]\n",
    "                                cleaned_doc.append(tokenized)\n",
    "                    \n",
    "                    score = int(cleaned[0])\n",
    "                    id = int(cleaned[1])\n",
    "                    scores.append(score)\n",
    "                    ids.append(id)\n",
    "                    \n",
    "                    if (score<=4) or (score>=7):\n",
    "                        idx_binary.append(doc_count)\n",
    "                        if (score<4):\n",
    "                            scores_binary.append(0)\n",
    "                        else:\n",
    "                            scores_binary.append(1)\n",
    "                    \n",
    "                    if (i != 0):\n",
    "                        filtered_doc = list(filter(None, cleaned_doc))\n",
    "                        padded_doc = pad_sequences(filtered_doc, maxlen=max_sent_len)\n",
    "\n",
    "                        cleaned_all.append(padded_doc.tolist())\n",
    "                        \n",
    "                        #DEBUGCODE\n",
    "                        if len(cleaned_all) in [114,116,135]:\n",
    "                            print(str(len(cleaned_all))+' exit at len=2 for sent<=max') \n",
    "                        cleaned_doc=[]\n",
    "\n",
    "                    doc_count += 1\n",
    "                    sent_counter = 0\n",
    "                   \n",
    "                    continue\n",
    "#             #DEBUGCODE        \n",
    "#             if i in [91,92,94]:\n",
    "#                 print(str(i)+' exit at all len for sent<max')\n",
    "            if (len(cleaned_doc) <= max_doc_len-1):\n",
    "                if cleaned != []:\n",
    "                    tokenized, isEmpty =tokenizeListOfTexts(cleaned, vocab)\n",
    "                    if not isEmpty:\n",
    "                        cleaned_doc.append(tokenized)\n",
    "            \n",
    "        else:\n",
    "            if (len(cleaned) == 2): # and (i != 0):\n",
    "#                 #DEBUGCODE\n",
    "#                 if i in [91,92,94]:\n",
    "#                     print(str(i)+' exit at len = 2 for sent=>max')\n",
    "                \n",
    "                if  (cleaned[0].isnumeric() and cleaned[1].isnumeric()):\n",
    "                    \n",
    "                    score = int(cleaned[0])\n",
    "                    id = int(cleaned[1])\n",
    "                    scores.append(score)\n",
    "                    ids.append(id)\n",
    "                    \n",
    "                    if (score<=4) or (score>=7):\n",
    "                        idx_binary.append(doc_count)\n",
    "                        if (score<4):\n",
    "                            scores_binary.append(0)\n",
    "                        else:\n",
    "                            scores_binary.append(1)\n",
    "                    \n",
    "                    if (i != 0):\n",
    "                        filtered_doc = list(filter(None, cleaned_doc))\n",
    "                        padded_doc = pad_sequences(filtered_doc, maxlen=max_sent_len)\n",
    "                        \n",
    "                        cleaned_all.append(padded_doc.tolist())\n",
    "                        \n",
    "                        #DEBUGCODE\n",
    "                        if len(cleaned_all) in [114,116,135]:\n",
    "                            print(str(len(cleaned_all))+' exit at len=2 for sent>max') \n",
    "                            print(cleaned_doc)\n",
    "                        cleaned_doc=[]\n",
    "                        doc_count += 1\n",
    "                        \n",
    "                    sent_counter = 0\n",
    "                    \n",
    "#             #DEBUGCODE\n",
    "#             if i in [91,92,94]:\n",
    "#                 print(str(i)+' exit at all len for sent=>max') \n",
    "#                 print(sent_counter)\n",
    "            \n",
    "        if (i%1000000 == 0) and (i!=0):\n",
    "            print(str(i)+' out of' + str(length))\n",
    "            #DEBUG CODE\n",
    "            #break\n",
    "            # print(cleaned_lines[i-500:i])\n",
    "            \n",
    "\n",
    "    \n",
    "    # Append the last doc \n",
    "    \n",
    "    while (sent_counter < max_doc_len):\n",
    "        tokenized = [0]\n",
    "        cleaned_doc.append(tokenized)\n",
    "        sent_counter += 1\n",
    "        \n",
    "    filtered_doc = list(filter(None, cleaned_doc))\n",
    "    padded_doc = pad_sequences(filtered_doc, maxlen=max_sent_len)\n",
    "    \n",
    "    cleaned_all.append(padded_doc.tolist())\n",
    "        \n",
    "        # cleaned_doc = cleaned_doc[0]\n",
    "    # \n",
    "    # del cleaned_all[0]\n",
    "    # del scores[0]\n",
    "    # del idx_binary[0]\n",
    "    # del ids[0]\n",
    "    features = np.asarray(cleaned_all)\n",
    "    labels = np.asarray(scores)\n",
    "    idx = np.asarray(idx_binary)\n",
    "    \n",
    "   \n",
    "    \n",
    "    if (willReturn == True):\n",
    "        print('cleanAndTokenizeEDUs Returned and Done')\n",
    "        return features, labels, idx, ids\n",
    "    else:\n",
    "        np.save(save_path + save_name + '_features'+ '.npy', features)\n",
    "        np.save(save_path + save_name + '_labels' + '.npy', labels)\n",
    "        np.save(save_path + save_name + '_idx_binary_only' + '.npy', idx)\n",
    "        np.save(save_path + save_name + '_doc_ids' + '.npy', ids)\n",
    "        np.save(save_path + save_name + '_scores_binary' + '.npy', scores_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 1 to initiate reload of google word2vec: 0\n",
      "114 exit at len=2 for sent<=max\n",
      "116 exit at len=2 for sent<=max\n",
      "135 exit at len=2 for sent<=max\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-df25473ce73c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanAndTokenizeEDUs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'imdb-edus.train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwillReturn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwillReturn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-24a698261ecc>\u001b[0m in \u001b[0;36mcleanAndTokenizeEDUs\u001b[0;34m(vocab, load_path, load_name, save_path, save_name, willReturn, max_sent_len, max_doc_len)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                         \u001b[0mfiltered_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaned_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                         \u001b[0mpadded_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sent_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                         \u001b[0mcleaned_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "isReload = int(input(\"Type 1 to initiate reload of google word2vec: \"))\n",
    "    \n",
    "if isReload:\n",
    "    print('word2vec model now loading')\n",
    "    model_word2vec_temp = gensim.models.KeyedVectors.load_word2vec_format('/home/owner/デスクトップ/milnet+edu/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "    model_word2vec = model_word2vec_temp\n",
    "    print('word2vec model loaded')\n",
    "\n",
    "load_path = '/home/owner/デスクトップ/milnet+edu/data/'  \n",
    "save_path = '/home/owner/デスクトップ/milnet+edu/data/Preprocessed/'\n",
    "\n",
    "load_name = ['imdb-edus.train', 'imdb-edus.test','imdb-edus.dev']\n",
    "save_name = ['train_data','test_data','validation_data']\n",
    "name_trailers = ['_features','_labels','_idx_binary_only','_doc_ids']\n",
    "willReturn = True\n",
    "\n",
    "vocab = model_word2vec.vocab\n",
    "\n",
    "\n",
    "features, scores, idx, ids = cleanAndTokenizeEDUs(vocab, load_path=load_path, load_name = 'imdb-edus.train', save_path=save_path,save_name=save_name[0], willReturn = willReturn)\n",
    "\n",
    "\n",
    "i=0\n",
    "#cleanAndTokenizeEDUs(model_word2vec.vocab, load_path=load_path, load_name = load_name[i], save_path=save_path,save_name=save_name[i], willSave = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(features)):\n",
    "    #print(len(features[i]))\n",
    "    if len(features[i])!=6:\n",
    "            print('i:'+str(i)+'   ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "            print(len(features[i]))\n",
    "    for j in range(len(features[i])):\n",
    "        \n",
    "        \n",
    "        #print(len(features[i][j]))\n",
    "        if len(features[i][j])!= 15:\n",
    "            print('i:'+str(i)+'j:'+str(j)+'   ************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = readEduStyledFiles(load_path,load_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 1 to initiate reload of google word2vec: 0\n",
      "word2vec model loaded\n",
      "114 exit at len=2 for sent<=max\n",
      "135 exit at len=2 for sent<=max\n",
      "116 exit at len=2 for sent<=max\n",
      "114 exit at len=2 for sent<=max\n",
      "116 exit at len=2 for sent<=max\n",
      "135 exit at len=2 for sent<=max\n",
      "1000000 out of11039529\n",
      "114 exit at len=2 for sent<=max\n",
      "116 exit at len=2 for sent<=max\n",
      "135 exit at len=2 for sent<=max\n",
      "1000000 out of1349298\n",
      "2000000 out of11039529\n",
      "1000000 out of1335054\n",
      "3000000 out of11039529\n",
      "4000000 out of11039529\n",
      "5000000 out of11039529\n",
      "6000000 out of11039529\n",
      "7000000 out of11039529\n",
      "8000000 out of11039529\n",
      "9000000 out of11039529\n",
      "10000000 out of11039529\n",
      "11000000 out of11039529\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #data=(関数, 引数)\n",
    "    isReload = int(input(\"Type 1 to initiate reload of google word2vec: \"))\n",
    "    \n",
    "    if isReload:\n",
    "        print('word2vec model now loading')\n",
    "        model_word2vec_temp = gensim.models.KeyedVectors.load_word2vec_format('/home/owner/デスクトップ/milnet+edu/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "        model_word2vec = model_word2vec_temp\n",
    "\n",
    "    print('word2vec model loaded')\n",
    "\n",
    "    load_path = '/home/owner/デスクトップ/milnet+edu/data/'  \n",
    "    save_path = '/home/owner/デスクトップ/milnet+edu/data/Preprocessed/'\n",
    "      \n",
    "    load_name = ['imdb-edus.train', 'imdb-edus.test','imdb-edus.dev']\n",
    "    save_name = ['train_data','test_data','validation_data']\n",
    "    name_trailers = ['_features','_labels','_idx_binary_only','_doc_ids']\n",
    "    willReturn = False\n",
    "    \n",
    "\n",
    "    #for i in range(len(load_name)):\n",
    "    #i = 0\n",
    "    #a,b,c,d = cleanAndTokenizeEDUs(model_word2vec.vocab, load_path=load_path, load_name = load_name[i], save_path=save_path,save_name=save_name[i], willSave = False)\n",
    "    \n",
    "    # features, scores, idx, ids = cleanAndTokenizeEDUs(model_word2vec.vocab, load_path=load_path, load_name = 'imdb-edus.train', save_path=save_path,save_name=save_name[0], willReturn = False)\n",
    "    \n",
    "    vocab = model_word2vec.vocab\n",
    "    data = [ ( cleanAndTokenizeEDUs, vocab, load_path, load_name[i], save_path, save_name[i] , willReturn) for i in range( len(load_name) ) ]\n",
    "    with Pool( multi.cpu_count()-1 ) as p:\n",
    "        p.map( wrapper_func_six, data )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Vocab List\n",
      "finished generating vocab\n"
     ]
    }
   ],
   "source": [
    "print('Starting Vocab List')\n",
    "docs = [save_path+name+'_features'+'.npy' for name in save_name]\n",
    "vocab_list = generateUniqueIndices(docs)\n",
    "print('finished generating vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421928"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Done\n"
     ]
    }
   ],
   "source": [
    "vocab_list = sorted(set(vocab_list))\n",
    "np.save(save_path + 'vocab_idx.npy', vocab_list)\n",
    "\n",
    "print('All Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(save_path + 'weights.npy', model_word2vec.wv.syn0[vocab_list])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_features = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/train_data_features.npy')\n",
    "valid_features = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/validation_data_features.npy')\n",
    "\n",
    "test_features = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/test_data_features.npy')\n",
    "test_labels = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/test_data_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5a8ad718338f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_word2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(train_features)+len(test_features)+len(valid_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "output = set()\n",
    "trends = [1,2,6,8,9,7,6,5,4,3,4,5,]\n",
    "for x in trends:\n",
    "    output.add(x)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421928\n"
     ]
    }
   ],
   "source": [
    "len(model_word2vec_temp.wv.syn0[vocab_list])\n",
    "print(len(vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list = np.asarray(vocab_list)\n",
    "np.save(save_path + save_name + '_word_indices' + '.npy', vocab_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
