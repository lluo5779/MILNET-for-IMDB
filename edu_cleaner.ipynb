{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/owner/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import string\n",
    "from numpy import loadtxt\n",
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import glob\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as multi\n",
    "sys.path.append(os.pardir)\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "\n",
    "# lines = loadtxt(\"/home/louis/SharedWindows/edu_data/\"+\"imdb-edus.train\")\n",
    "# train = []\n",
    "\n",
    "# with open(dir+\"imdb-edus.train\", 'r') as train_file:\n",
    "#     lines = train_file.readlines()\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "# '''text2index'''\n",
    "# \n",
    "# from func import TextDeal\n",
    "# from func import wrapper_func_four\n",
    "# \n",
    "# vocab = wList\n",
    "# \n",
    "# td = TextDeal()\n",
    "# load_path = sorted( glob.glob( '/home/dl-box/デスクトップ/PythonFile/hirose/Text/text_list/*.txt' ) )\n",
    "# name = [ os.path.splitext( os.path.basename( r ) )[0] for r in load_path ]\n",
    "# save_path = '/home/dl-box/デスクトップ/PythonFile/hirose/LSTM/W2I/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##\n",
    "\n",
    "def wrapper_func_six(tuple_data):\n",
    "    return(tuple_data[0](tuple_data[1],tuple_data[2],tuple_data[3],tuple_data[4], tuple_data[5], tuple_data[6]) )\n",
    "\n",
    "def readEduStyledFiles(dir, filename):\n",
    "    with open(dir+filename, 'r') as train_file:\n",
    "        lines = train_file.readlines()\n",
    "    return lines\n",
    "        \n",
    "def tokenizeListOfTexts(list, vocab):\n",
    "    idx = []\n",
    "    for word in list:\n",
    "        if word in vocab:\n",
    "            idx.append(vocab[word].index)\n",
    "            #print(word)\n",
    "        # else:\n",
    "        #     print(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def cleanAndTokenizeEDUs(vocab, load_path=None, load_name=None, save_path=None, save_name=None, willReturn = True, maxlen = 15):\n",
    "    \n",
    "    if (willReturn == False) and (all(v is None for v in [load_path, load_name, save_path, save_name])):\n",
    "        raise ValueError('Function Arguments cannot all be None Type when saving files')\n",
    "    elif (willReturn == True) and (all(v is None for v in [load_path, load_name])):\n",
    "        raise ValueError('Load arguments cannot all be None Type when loading file')\n",
    "    \n",
    "    lines = readEduStyledFiles(load_path,load_name)\n",
    "\n",
    "    cleaned_lines = []\n",
    "    length = len(lines)\n",
    "    scores = []\n",
    "    ids = []\n",
    "    cleaned_doc=[]\n",
    "    cleaned_all=[]\n",
    "    idx_binary = [] # index for binary training only\n",
    "    doc_count = -1\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        cleaned = lines[i].replace('\\n','').replace('<s>','').replace(\"'\",'')\n",
    "        cleaned = text_to_word_sequence(cleaned)\n",
    "        \n",
    "        \n",
    "        if (len(cleaned) == 2): # and (i != 0):\n",
    "            if  (cleaned[0].isnumeric() and cleaned[1].isnumeric()):\n",
    "                score = int(cleaned[0])\n",
    "                id = int(cleaned[1])\n",
    "                if (score<=4) or (score>=7):\n",
    "                    idx_binary.append(doc_count)\n",
    "                filtered_doc = list(filter(None, cleaned_doc))\n",
    "                padded_doc = pad_sequences(filtered_doc, maxlen=maxlen)\n",
    "                scores.append(score)\n",
    "                ids.append(id)\n",
    "                cleaned_all.append(padded_doc)\n",
    "                cleaned_doc=[]\n",
    "                doc_count += 1\n",
    "                continue\n",
    "        \n",
    "        tokenized=tokenizeListOfTexts(cleaned, vocab)\n",
    "        \n",
    "        if (i%1000000 == 0) and (i!=0):\n",
    "            print(str(i)+' out of' + str(length))\n",
    "            #break\n",
    "            # print(cleaned_lines[i-500:i])\n",
    "    \n",
    "        cleaned_doc.append(tokenized)\n",
    "        # cleaned_doc = cleaned_doc[0]\n",
    "    \n",
    "    del cleaned_all[0]\n",
    "    del scores[0]\n",
    "    del idx_binary[0]\n",
    "    del ids[0]\n",
    "    features = np.asarray(cleaned_all)\n",
    "    labels = np.asarray(scores)\n",
    "    idx = np.asarray(idx_binary)\n",
    "    \n",
    "   \n",
    "    \n",
    "    if (willReturn == True):\n",
    "        print('cleanAndTokenizeEDUs Returned and Done')\n",
    "        return features, labels, idx, ids\n",
    "    else:\n",
    "        np.save(save_path + save_name + '_features'+ '.npy', features)\n",
    "        np.save(save_path + save_name + '_labels' + '.npy', labels)\n",
    "        np.save(save_path + save_name + '_idx_binary_only' + '.npy', idx)\n",
    "        np.save(save_path + save_name + '_doc_ids' + '.npy', ids)\n",
    "        print(save_name + ' cleanAndTokenizeEDUs Saved and Done')\n",
    "\n",
    "dir = '/home/louis/SharedWindows/edu_data/'  \n",
    "\n",
    "def getWordIndices()\n",
    "# t = Tokenizer()\n",
    "\n",
    "# tokenized = t.fit_on_texts(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 1 to initiate reload of google word2vec: 0\n",
      "word2vec model loaded\n",
      "1000000 out of11039529\n",
      "1000000 out of1349298\n",
      "1000000 out of1335054\n",
      "test_data cleanAndTokenizeEDUs Saved and Done\n",
      "2000000 out of11039529\n",
      "validation_data cleanAndTokenizeEDUs Saved and Done\n",
      "3000000 out of11039529\n",
      "4000000 out of11039529\n",
      "5000000 out of11039529\n",
      "6000000 out of11039529\n",
      "7000000 out of11039529\n",
      "8000000 out of11039529\n",
      "9000000 out of11039529\n",
      "10000000 out of11039529\n",
      "11000000 out of11039529\n",
      "train_data cleanAndTokenizeEDUs Saved and Done\n",
      "All Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #data=(関数, 引数)\n",
    "    isReload = int(input(\"Type 1 to initiate reload of google word2vec: \"))\n",
    "    \n",
    "    if isReload:\n",
    "        model_word2vec_temp = gensim.models.KeyedVectors.load_word2vec_format('/home/owner/デスクトップ/milnet+edu/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "        model_word2vec = model_word2vec_temp\n",
    "\n",
    "    print('word2vec model loaded')\n",
    "\n",
    "    load_path = '/home/owner/デスクトップ/milnet+edu/data/'  \n",
    "    save_path = '/home/owner/デスクトップ/milnet+edu/data/Preprocessed/'\n",
    "      \n",
    "    load_name = ['imdb-edus.train', 'imdb-edus.test','imdb-edus.dev']\n",
    "    save_name = ['train_data','test_data','validation_data']\n",
    "    \n",
    "    willReturn = False\n",
    "    \n",
    "    # cleanAndTokenizeEDUs(model_word2vec.vocab, load_path=load_path, load_name = 'imdb-edus.train', save_path=save_path,save_name=save_name[0], willReturn = False)\n",
    "    # features, scores, idx, ids = cleanAndTokenizeEDUs(model_word2vec.vocab, load_path=load_path, load_name = 'imdb-edus.train', save_path=save_path,save_name=save_name[0], willReturn = False)\n",
    "    \n",
    "    vocab = model_word2vec.vocab\n",
    "    data = [ ( cleanAndTokenizeEDUs, vocab, load_path, load_name[i], save_path, save_name[i] , willReturn) for i in range( len(load_name) ) ]\n",
    "    with Pool( multi.cpu_count()-1 ) as p:\n",
    "        p.map( wrapper_func_six, data )\n",
    "        \n",
    "    print('All Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.load('/home/owner/デスクトップ/milnet+edu/data/Preprocessed/test_data_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-3d6f777cac8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'avg' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1291504e-03, -8.9645386e-04,  3.1852722e-04, ...,\n",
       "        -1.5640259e-03, -1.2302399e-04, -8.6307526e-05],\n",
       "       [ 7.0312500e-02,  8.6914062e-02,  8.7890625e-02, ...,\n",
       "        -4.7607422e-02,  1.4465332e-02, -6.2500000e-02],\n",
       "       [-1.1779785e-02, -4.7363281e-02,  4.4677734e-02, ...,\n",
       "         7.1289062e-02, -3.4912109e-02,  2.4169922e-02],\n",
       "       ...,\n",
       "       [-1.9653320e-02, -9.0820312e-02, -1.9409180e-02, ...,\n",
       "        -1.6357422e-02, -1.3427734e-02,  4.6630859e-02],\n",
       "       [ 3.2714844e-02, -3.2226562e-02,  3.6132812e-02, ...,\n",
       "        -8.8500977e-03,  2.6977539e-02,  1.9042969e-02],\n",
       "       [ 4.5166016e-02, -4.5166016e-02, -3.9367676e-03, ...,\n",
       "         7.9589844e-02,  7.2265625e-02,  1.3000488e-02]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word2vec_temp.wv.syn0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
